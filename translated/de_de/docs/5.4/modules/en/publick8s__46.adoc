= Bereitstellung von SUSE Security in der Public Cloud
:revdate: 2025-06-25
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/08.publick8s/08.publick8s.md
:page-opendocs-slug: /deploying/publick8s

== Bereitstellung von {product-name} auf einem öffentlichen Cloud-Kubernetes-Dienst

Stellen Sie {product-name} auf jedem Public Cloud K8s Service wie AWS EKS, Azure AKS, IBM Cloud K8s, Google Cloud, Alibaba Cloud oder Oracle Cloud bereit.{product-name} hat das Amazon EKS Anywhere Conformance and Validation Framework bestanden und ist somit eine validierte Lösung und als Add-on für EKS-Anywhere auf Snowball Edge-Geräten über die AWS-Konsole verfügbar.

Erstellen Sie zunächst Ihren K8s-Cluster und bestätigen Sie den Zugriff mit `+kubectl get nodes+`.

Verwenden Sie für die Bereitstellung von {product-name} die Beispielanweisungen und -beispiele aus dem Abschnitt "Kubernetes" des Kapitels "Produktionsbereitstellung". Bearbeiten Sie das Beispiel yaml, wenn Sie {product-name} Bilder aus einem lokalen oder Cloud-Register wie ECR oder ACR beziehen.

Einige Cloud-Anbieter verfügen über integrierte Load-Balancer, die einfach durch die Verwendung von `+Type: LoadBalancer+` anstelle von NodePort für die {product-name} webui eingesetzt werden können.

{product-name} unterstützt auch die Helm-basierte Bereitstellung mit einem Helm-Diagramm unter https://github.com/neuvector/neuvector-helm.

=== Netzzugang

Stellen Sie sicher, dass der interne und externe Zugang richtig konfiguriert ist. Für den NodePort-Dienst muss der zufällige Port im Bereich 3xxxx auf einer öffentlichen IP eines Worker- oder Master-Knotens von außen erreichbar sein. Sie können auf die Konsole über die öffentliche IP-Adresse eines beliebigen Arbeitsknotens und diesen Port (NodePort) oder über die öffentliche IP-Adresse des Load Balancer und den Standardport 8443 zugreifen. Sie können die IP/Port mit anzeigen:

[,shell]
----
kubectl get svc -n neuvector
----

Die meisten K8s-Dienste aktivieren/erlauben automatisch die gesamte Inter-Pod-/Inter-Cluster-Kommunikation zwischen den Knoten, wodurch auch die {product-name} Container (Enforcer, Controller, Manager) innerhalb des Clusters kommunizieren können.

Die Kubernetes yaml-Beispieldatei wird einen Manager und 3 Controller bereitstellen. Es wird ein Enforcer auf jedem Knoten als Daemonset bereitgestellt. Anmerkung: Es wird nicht empfohlen, mehr als einen Manager hinter einem Load Balancer einzusetzen (zu skalieren), da es zu Problemen mit dem Sitzungsstatus kommen kann.

== Microsoft Azure AKS

Bei der Bereitstellung eines K8s-Clusters auf Azure ist die Standardeinstellung für Kubernetes-RBACs deaktiviert. Aktivieren Sie RBACs, um die Clusterrolle cluster-admin zu aktivieren, andernfalls müssen Sie diese später manuell erstellen, um Helm-basierte Einsätze zu unterstützen.

== Google Cloud-Plattform / GKE

Sie können die integrierten Load Balancer verwenden, die einfach zu implementieren sind, indem Sie &apos;`+Type: LoadBalancer+`&apos; anstelle von NodePort für die {product-name} webui verwenden. Die Konfiguration von persistentem Speicher mit dem Typ RWM (read write many) erfordert möglicherweise die Einrichtung eines Speicherdienstes wie NFS vor der Bereitstellung von {product-name}.

{product-name} erfordert ein SDN-Plug-in wie Flanell, Webstoff oder Kattun.

Verwenden Sie die Umgebungsvariable NV_PLATFORM_INFO mit dem Wert platform=Kubernetes:GKE, um {product-name} die Durchführung von GKE-spezifischen Aktionen zu ermöglichen, z. B. die Ausführung der GKE Kubernetes CIS Benchmarks.

== GKE Auto Pilot Unterstützung

Die Unterstützung von GKE Auto Pilot ist mit NeuVector v5.4.3 und höher verfügbar. Bitte folgen Sie den nachstehenden Schritten, um NeuVector auf dem Auto Pilot-Cluster einzusetzen.

Vor dem Einsatz von NeuVector sollte eine `+AllowlistSynchronizer+` auf dem Cluster erstellt werden. Hier ist die Konfigurations-YAML mit `+allowlistPath+` und der Befehl zum Anwenden der YAML:

Beispielbefehl zur Anwendung von YAML:

[,shell]
----
kubectl apply -f allowlist.yaml
----

Beispiel einer YAML-Konfiguration:

[,yaml]
----
apiVersion: auto.gke.io/v1
kind: AllowlistSynchronizer
metadata:
  name: neuvector-allowlist
spec:
  allowlistPaths:
  - SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
  - SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
----

Nachdem Sie den Befehl `+kubectl apply -f <YAML file>+` ausgeführt haben, prüfen Sie, ob die `+AllowlistSynchronizer+` bereit ist.

Beispielbefehl:

[,shell]
----
kubectl get AllowlistSynchronizer neuvector-allowlist -o yaml
----

Beispiel einer YAML-Konfiguration:

[,yaml]
----
apiVersion: auto.gke.io/v1
kind: AllowlistSynchronizer
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"auto.gke.io/v1","kind":"AllowlistSynchronizer","metadata":{"annotations":{},"name":"neuvector-allowlist"},"spec":{"allowlistPaths":["SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml","SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml"]}}
  creationTimestamp: "2025-04-28T18:17:16Z"
  generation: 1
  name: neuvector-allowlist
  resourceVersion: "13326"
  uid: 3e425c28-9bef-4459-b769-381d974f17f6
spec:
  allowlistPaths:
  - SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
  - SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
status:
  conditions:
  - lastTransitionTime: "2025-04-28T18:17:17Z"
    message: Synchronization completed successfully; allowlists up to date
    observedGeneration: 1
    reason: SyncSuccessful
    status: "True"
    type: Ready
  lastSyncAttempt: "2025-04-28T18:17:17Z"
  managedAllowlistStatus:
  - filePath: SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
    generation: 1
    lastSuccessfulSync: "2025-04-28T18:17:16Z"
    phase: Installed
  - filePath: SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
    generation: 1
    lastSuccessfulSync: "2025-04-28T18:17:17Z"
    phase: Installed
----

Die folgende `+override.yaml+` Datei muss verwendet werden, um NeuVector auf dem GKE Autopilot-Cluster einzusetzen, wenn Helm verwendet wird.

[,yaml]
----
cve:
  scanner:
    podLabels:
      # The scanner allowlist should be mapped with scanner deployment workload.
      cloud.google.com/matching-allowlist: suse-neuvector-scanner
    resources:
      # Below are the tested limits for scanner deployment in GKE Auto-Pilot cluster for scanner pod.
      limits:
        ephemeral-storage: "3Gi"
      requests:
        ephemeral-storage: "2Gi"
enforcer:
  podLabels:
     # The enforcer allowlist should be mapped with the enforcer daemon set workload.
    cloud.google.com/matching-allowlist: suse-neuvector-enforcer
----

Wenn Sie die YAML-Bereitstellung verwenden, fügen Sie bitte die `+podLabels+` und die Ressourcenlimits auf den YAML-Konfigurationen `+enforcer+` und `+scanner+` entsprechend hinzu.

Wenn Sie mehr über `+allowlistSynchronizer+` erfahren möchten, lesen Sie bitte die https://cloud.google.com/kubernetes-engine/docs/how-to/run-autopilot-partner-workloads[Dokumentation] GKE.

== Umgang mit Auto-Scaling-Knoten mit einem Budget für Pod-Unterbrechungen

Public-Cloud-Anbieter unterstützen die Fähigkeit zur automatischen Skalierung von Knoten, die Pods einschließlich der {product-name} Controller dynamisch evakuieren können. Um Unterbrechungen der Steuerungen zu vermeiden, kann ein {product-name} Pod-Störungsbudget erstellt werden.

Erstellen Sie z. B. die folgende Datei nv_pdr.yaml, um sicherzustellen, dass zu jedem Zeitpunkt mindestens 2 Controller in Betrieb sind.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

Dann

[,shell]
----
kubectl create -f nv_pdr.yaml
----

Für weitere Informationen: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
