= Mise à jour {product-name}
:revdate: 2024-09-27
:page-revdate: {revdate}
:page-opendocs-origin: /10.updating/01.updating/01.updating.md
:page-opendocs-slug:  /updating/updating

== Mise à jour des composants de {product-name} 

Il est très facile d'actualiser vos contenants {product-name}. Si une nouvelle version est disponible, tirez-la de Docker Hub. Il est recommandé d'utiliser une stratégie &apos;`+rolling update+`&apos; pour qu'au moins un conteneur Allinone ou Controller fonctionne à tout moment lors d'une mise à jour.

[IMPORTANT]
====
Les mises à jour du système d'exploitation de l'hôte, les redémarrages et les mises à jour de l'orchestrateur peuvent entraîner l'expulsion ou l'arrêt des pods. Si un contrôleur est affecté et qu'il n'y a pas d'autres contrôleurs actifs pour maintenir l'état, les contrôleurs peuvent devenir disponibles pendant un certain temps, le temps que de nouveaux contrôleurs soient démarrés, qu'un cluster soit formé avec un leader et que l'on tente d'accéder à la sauvegarde du stockage permanent de la configuration pour restaurer le cluster. Soyez prudent lorsque vous planifiez des mises à jour et des redémarrages d'hôtes ou d'orchestrateurs qui peuvent affecter le nombre de contrôleurs disponibles à tout moment. Voir le budget de perturbation des pods ci-dessous pour les moyens possibles d'atténuer ce problème.

Si le déploiement a été effectué à l'aide des tableaux {product-name} Helm, la mise à jour prendra en charge les services supplémentaires, les liaisons de rôles ou d'autres exigences de mise à niveau.

Si les mises à jour sont effectuées manuellement ou s'il n'y a qu'un seul Allinone ou Controller en cours d'exécution, veuillez noter que les données de connexion réseau actuelles ne sont PAS stockées et seront perdues lorsque le conteneur {product-name} sera arrêté.

{product-name} prend en charge les données persistantes pour la politique et la configuration de {product-name}. Ceci configure une sauvegarde en temps réel pour monter un volume dans /var/neuvector/. Le principal cas d'utilisation est lorsque le volume persistant est monté, la configuration et la politique sont stockées pendant l'exécution sur le volume persistant. En cas de défaillance totale du cluster, la configuration est automatiquement restaurée lors de la création du nouveau cluster. La configuration et la politique peuvent également être restaurées ou supprimées manuellement du volume /var/neuvector/.
====

[IMPORTANT]
====
Si un volume persistant n'est pas monté, {product-name} ne stocke PAS la configuration ou la stratégie en tant que données persistantes. Veillez à sauvegarder la configuration et la stratégie du contrôleur avant d'arrêter le conteneur allinone ou le conteneur de contrôleur. Cette opération peut être effectuée dans Settings -> Configuration. Le contrôleur peut également être déployé dans une configuration HA avec 3 ou 5 contrôleurs en fonctionnement, auquel cas la politique persistera avec les autres contrôleurs pendant la mise à jour de l'un d'entre eux.
====

Pour mettre à jour manuellement {product-name} à l'aide de docker-compose :

[,shell]
----
sudo docker-compose -f <filename> down
----

[TIP]
====
Si aucun nom de fichier n'est spécifié, le fichier docker-compose.yml est utilisé.
====

Assurez-vous que le fichier docker-compose.yml ou tout autre fichier approprié est édité avec la version de l'image désirée, si nécessaire, puis :

[,shell]
----
$sudo docker-compose -f <filename> up -d
----

[NOTE]
====
Nous recommandons que tous les composants de {product-name} soient mis à jour en même temps. La rétrocompatibilité est assurée pour au moins une version mineure en arrière. Bien que la plupart des anciennes versions soient rétrocompatibles, il peut y avoir des exceptions qui entraînent un comportement inattendu.
====

== Rolling Updates

Les outils d'orchestration tels que Kubernetes, RedHat OpenShift et Rancher prennent en charge les mises à jour en continu avec des politiques configurables. Vous pouvez utiliser cette fonction pour mettre à jour les conteneurs {product-name}. Le plus important est de s'assurer qu'il y a au moins un Allinone/Controller en fonctionnement afin que les politiques, les journaux et les données de connexion ne soient pas perdus. Veillez à ce qu'il y ait un minimum de 30 secondes entre les mises à jour des conteneurs afin qu'un nouveau chef puisse être élu et que les données soient synchronisées entre les contrôleurs.

=== Exemple de mise à jour continue de Kubernetes

Si votre Deployment ou Daemonset est déjà en cours d'exécution, vous pouvez modifier le fichier yaml pour le remplacer par la nouvelle version, puis appliquer la mise à jour :

[,shell]
----
kubectl apply -f <yaml file>
----

Pour mettre à jour une nouvelle version de {product-name} à partir de la ligne de commande.

[,shell]
----
kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:4.2.2 -n neuvector
kubectl set image deployment/neuvector-manager-pod neuvector-manager-pod=neuvector/manager:4.2.2 -n neuvector
kubectl set image DaemonSet/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:4.2.2 -n neuvector
----

Pour vérifier l'état de la mise à jour continue :

[,shell]
----
kubectl rollout status -n neuvector ds/neuvector-enforcer-pod
kubectl rollout status -n neuvector deployment/neuvector-controller-pod  # same for manager, scanner etc
----

Pour annuler la mise à jour :

[,shell]
----
kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod
kubectl rollout undo -n neuvector deployment/neuvector-controller-pod  # same for manager, scanner etc
----

== Mise à jour de la base de données CVE sur les vulnérabilités

L'image {product-name} Scanner est régulièrement mise à jour sur neuvector avec les nouvelles mises à jour de la base de données CVE, en utilisant la balise 'latest'.

Le déploiement par défaut de {product-name} comprend le déploiement de pods de scanners ainsi qu'une tâche cron Updater pour mettre à jour les scanners tous les jours.

Pour plus de détails, voir la section xref:updating.adoc[Mise à jour de la base de données CVE].

La version de la base de données CVE peut être consultée dans la console, dans l'onglet Vulnérabilités. Vous pouvez également inspecter l'image du conteneur Updater. Le dernier numéro de version de la base de données peut également être trouvé dans la liste https://raw.githubusercontent.com/neuvector/manifests/main/versions/scanner[ici.]

[,shell]
----
docker inspect neuvector/updater
----

[,json]
----
"Labels": {
                "neuvector.image": "neuvector/updater",
                "neuvector.role": "updater",
                "neuvector.vuln_db": "1.255"
            }
----

Vous pouvez également rechercher la "version" dans les journaux du contrôleur/allinone. Par exemple dans Kubernetes :

[,bash]
----
kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version
----

[,shell]
----
2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576
----

== Budget pour la perturbation des pods

Une fonctionnalité de Kubernetes permet de s'assurer qu'un nombre minimum de contrôleurs fonctionne à tout moment. Cette fonction est utile pour la vidange des nœuds ou d'autres activités d'entretien susceptibles de supprimer des gousses de contrôleur. Par exemple, créez et appliquez le fichier ci-dessous nv_pdb.yaml pour vous assurer qu'au moins deux contrôleurs fonctionnent à tout moment.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

== Mise à jour de {product-name} 4.x à 5.1.x

Mettez d'abord à jour vers une version 5.1.x telle que 5.1.3, puis consultez la xref:kubernetes.adoc[section sur le déploiement de Kubernetes] pour mettre à jour vers la version 5.2.x+ pour les changements importants apportés aux comptes de services et aux liaisons.

Pour les utilisateurs de Helm, mettez à jour vers {product-name} Helm chart 2.0.0 ou plus récent (avant {product-name} 5.2.0). Si vous mettez à jour une installation Operator ou Helm sur OpenShift, voir la note ci-dessous.

. Supprimer l'ancien clusterrole neuvector-binding-customresourcedefinition
+
[,shell]
----
kubectl delete clusterrole neuvector-binding-customresourcedefinition
----

. Appliquer le nouveau verbe de mise à jour pour neuvector-binding-customresourcedefinition clusterrole
+
[,shell]
----
kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
----

. Supprimer l'ancien schéma crd pour Kubernetes 1.19+.
+
[,shell]
----
kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml
----

. Créer un nouveau schéma crd pour Kubernetes 1.19+.
+
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml
----

. Créer un nouveau rôle de cluster DLP, WAP, Admission et un nouveau lien de cluster (clusterrolebinding)
+
[,shell]
----
kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default
kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default
kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default
----

. Mise à jour des noms et chemins d'accès des images {product-name} depuis le hub Docker (docker.io).
Les images sont sur le registre {product-name} Docker Hub. Utilisez la balise de version appropriée pour le gestionnaire, le contrôleur, l'exécuteur, et laissez la version "dernière" pour l'analyseur et l'actualisateur. Par exemple :
* neuvector/manager:5.1.3
* neuvector/controller:5.1.3
* neuvector/enforcer:5.1.3
* neuvector/scanner:latest
* neuvector/updater:latest

Optionnellement, supprimez toutes les références à la licence et aux secrets de {product-name} dans les graphiques Helm, le yaml de déploiement, le configmap, les scripts, etc., car ils ne sont plus nécessaires pour extraire les images ou pour commencer à utiliser {product-name}.

=== Note sur le SCC et la mise à niveau via Operator/Helm

Privileged SCC est ajouté au compte de service spécifié dans le yaml de déploiement par Operator version 1.3.4 et supérieure dans les nouveaux déploiements. Dans le cas d'une mise à niveau de l'Opérateur {product-name} d'une version précédente vers 1.3.4 ou de Helm vers 2.0.0, veuillez supprimer Privileged SCC avant de procéder à la mise à niveau.

[,shell]
----
oc delete rolebinding -n neuvector system:openshift:scc:privileged
----
