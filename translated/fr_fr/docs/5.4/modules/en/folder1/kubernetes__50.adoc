= Kubernetes
:revdate: 2025-06-05
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/02.kubernetes/02.kubernetes.md
:page-opendocs-slug: /déploiement/kubernetes

== Déploiement à l'aide de Kubernetes

Vous pouvez utiliser Kubernetes pour déployer des conteneurs distincts pour le gestionnaire, le contrôleur et l'exécuteur et vous assurer que tous les nouveaux nœuds disposent d'un exécuteur déployé. {product-name} nécessite et prend en charge les plugins de réseau Kubernetes tels que flannel, weave ou calico.

Le fichier d'exemple déploie un gestionnaire et trois contrôleurs. Il déploiera un enforcer sur chaque nœud en tant que daemonset. Par défaut, l'exemple ci-dessous sera également déployé sur le nœud maître.

Voir la section inférieure pour spécifier des nœuds de gestionnaires ou de contrôleurs dédiés à l'aide d'étiquettes de nœuds. 

[NOTE]
====
Il n'est pas recommandé de déployer (mettre à l'échelle) plus d'un gestionnaire derrière un équilibreur de charge en raison des problèmes potentiels liés à l'état de la session. Si vous prévoyez d'utiliser une revendication PersistentVolume pour stocker la sauvegarde des fichiers de configuration de {product-name}, veuillez consulter la section générale Sauvegarde/Données persistantes dans l'aperçu du xref:production.adoc#_backups_and_persistent_data[déploiement de {product-name}].
====

Si votre déploiement prend en charge un équilibreur de charge intégré, remplacez le type NodePort par LoadBalancer pour la console dans le fichier yaml ci-dessous.

{product-name} prend en charge le déploiement basé sur Helm avec un diagramme Helm à l'adresse https://github.com/neuvector/neuvector-helm.

Il existe une section distincte pour les instructions relatives à OpenShift, et Docker EE on Kubernetes comporte des étapes spéciales décrites dans la section Docker.

=== {product-name} Images sur Docker Hub

Les images se trouvent sur le registre Docker Hub de {product-name}. Utilisez la balise de version appropriée pour le gestionnaire, le contrôleur, l'exécuteur, et laissez la version "dernière" pour l'analyseur et l'actualisateur. Par exemple :

* neuvector/manager:5.4.3
* neuvector/controller:5.4.3
* neuvector/enforcer:5.4.3
* neuvector/scanner:latest
* neuvector/updater:latest

Veillez à mettre à jour les références des images dans les fichiers yaml appropriés.

Si vous déployez avec la charte actuelle de {product-name} Helm (v1.8.9+), les modifications suivantes doivent être apportées au fichier values.yml :

* Mettre à jour le registre vers docker.io
* Mettre à jour les noms/étiquettes de l'image avec la version actuelle sur le hub Docker, comme indiqué ci-dessus.
* Laisser le champ imagePullSecrets vide

[NOTE]
====
En cas de déploiement à partir de la charte Rancher Manager 2.6.5+ {product-name}, les images sont extraites automatiquement du repo d'images miroirs du Rancher Registry, et déployées dans l'espace de noms cattle-neuvector-system.
====


== Déployer {product-name}

. Créez l'espace de noms {product-name} et les comptes de service requis : 
+
--
[,shell]
----
kubectl create namespace neuvector
kubectl create sa controller -n neuvector
kubectl create sa enforcer -n neuvector
kubectl create sa basic -n neuvector
kubectl create sa updater -n neuvector
kubectl create sa scanner -n neuvector
kubectl create sa registry-adapter -n neuvector
kubectl create sa cert-upgrader -n neuvector
----
--
. *(Facultatif*) Créez l'admission de sécurité du pod (PSA) ou la politique de sécurité du pod (PSP) sur le site {product-name}. Si vous avez activé Pod Security Admission (aka Pod Security Standards) dans Kubernetes 1.25+, ou Pod Security Policies (avant 1.25) dans votre cluster Kubernetes, ajoutez ce qui suit pour {product-name} (par exemple, nv_psp.yaml). 
+
[NOTE]
====
* PSP est obsolète dans Kubernetes 1.21 et sera totalement supprimé dans la version 1.25. 
* Les pods Manager et Scanner s'exécutent sans uid. Si votre PSP a une règle `+Run As User: Rule: MustRunAsNonRoot+`, ajoutez ce qui suit à l'exemple de fichier yaml ci-dessous (avec la valeur appropriée pour `+###+`) :
====
+
--
[,yaml]
----
securityContext:
    runAsUser: ###
----

Pour PSA dans Kubernetes 1.25+, étiqueter l'espace de noms {product-name} avec un profil privilégié pour le déploiement sur un cluster compatible avec PSA. 
[,shell]
----
kubectl label namespace neuvector "pod-security.kubernetes.io/enforce=privileged" 
----
--
. Créer les ressources personnalisées (CRD) pour les règles de sécurité de {product-name}. Pour Kubernetes 1.19+ : 
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
----
--
. Ajouter l'autorisation de lecture pour accéder à l'API kubernetes. 
+
[IMPORTANT]
====
Le déploiement standard de {product-name} 5.2+ utilise les comptes de service les moins privilégiés au lieu des comptes par défaut. Voir ci-dessous si vous effectuez une mise à jour à partir d'une version antérieure à la 5.3. 
====
+
[WARNING]
====
Si vous passez à la version 5.3.0+, exécutez les commandes suivantes en fonction de votre version actuelle :
====
+
--
[tabs]
======
Version 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules 
----
====

Versions antérieures à 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules
kubectl delete rolebinding neuvector-admin -n neuvector 
----
====
======

Appliquez les autorisations de lecture via les commandes "create clusterrole" suivantes : 

[,shell]
----
kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io
kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller
kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller --serviceaccount=neuvector:enforcer --serviceaccount=neuvector:scanner --serviceaccount=neuvector:registry-adapter -n neuvector
kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller 
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
kubectl create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
kubectl create rolebinding neuvector-binding-cert-upgrader --role=neuvector-binding-cert-upgrader --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create rolebinding neuvector-binding-job-creation --role=neuvector-binding-job-creation --serviceaccount=neuvector:controller -n neuvector
kubectl create rolebinding neuvector-binding-lease --role=neuvector-binding-lease --serviceaccount=neuvector:controller --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create clusterrole neuvector-binding-nvgroupdefinitions --verb=list,get,delete --resource=nvgroupdefinitions
kubectl create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
----
--
. Exécutez les commandes suivantes pour vérifier si les comptes de service neuvector/controller et neuvector/updater ont été ajoutés avec succès. 
+
--
[,shell]
----
kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvgroupdefinitions -o wide
----

Exemple de sortie : 

[,shell]
----
NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               45s                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              45s                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         44s                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          44s                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   43s                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                43s                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                43s                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   43s                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                43s                    neuvector/controller
neuvector-binding-nvgroupdefinitions                ClusterRole/neuvector-binding-nvgroupdefinitions                40s                    neuvector/controller
----

Et ce commandement : 

[,shell]
----
kubectl get RoleBinding neuvector-binding-scanner neuvector-binding-cert-upgrader neuvector-binding-job-creation neuvector-binding-lease neuvector-binding-secret -n neuvector -o wide
----

Exemple de sortie : 

[,shell]
----
NAME                              ROLE                                   AGE    USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         8m8s                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   8m8s                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    8m8s                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           8m8s                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          8m8s                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter
----
--
. *(Facultatif*) Créez le maître de la fédération et/ou les services de gestion multi-clusters à distance. Si vous prévoyez d'utiliser les fonctions de gestion multi-clusters dans {product-name}, un cluster doit avoir le service Federation Master déployé, et chaque cluster distant doit avoir le service Federation Worker. Pour plus de souplesse, vous pouvez choisir de déployer les services Maître et Travailleur sur chaque cluster, de sorte que n'importe quel cluster puisse être maître ou distant. Gestion des clusters fédérés 
+
--
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Créez ensuite le(s) service(s) approprié(s) : 

[,shell]
----
kubectl create -f nv_master_worker.yaml 
----
--
. Créez les services et les pods primaires de {product-name} en utilisant les commandes de version prédéfinies ou modifiez l'exemple yaml ci-dessous. La version prédéfinie invoque un LoadBalancer pour la console {product-name}. Si vous utilisez l'exemple de fichier yaml ci-dessous, remplacez les noms des images et les balises <version> pour les références des images du gestionnaire, du contrôleur et de l'exécuteur dans le fichier yaml. Apportez également toutes les autres modifications nécessaires à votre environnement de déploiement (telles que LoadBalancer/NodePort/Ingress pour l'accès au gestionnaire, etc.) Le fichier YAML ci-dessous doit être modifié pour les changements de certificats internes si le déploiement se fait à partir de la version 5.4.2 ou d'une version plus récente. Se référer à ce <<_kubernetes_deployment_yaml_for_v5_4_2_onwards,YAML>>.
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-k8s.yaml 
----

Ou, si vous modifiez l'un des yaml ou échantillons ci-dessus à partir de ce qui suit : 

[,shell]
----
kubectl create -f neuvector.yaml 
----

C'est tout ! Vous devriez pouvoir vous connecter à la console {product-name} et vous connecter avec admin:admin, par exemple `+https://<public-ip>:8443+`
--

[NOTE]
====
Le service nodeport spécifié dans le fichier neuvector.yaml ouvrira un port aléatoire sur tous les nœuds kubernetes pour le port de la console web de gestion {product-name}. Vous pouvez également utiliser un LoadBalancer ou Ingress, en utilisant une adresse IP publique et le port par défaut 8443. Pour nodeport, assurez-vous d'ouvrir l'accès à travers les pare-feu pour ce port, si nécessaire. Si vous souhaitez voir quel port est ouvert sur les nœuds hôtes, exécutez les commandes suivantes :

[,shell]
----
kubectl get svc -n neuvector
----

Et vous verrez quelque chose comme :

[,shell]
----
NAME                          CLUSTER-IP      EXTERNAL-IP   PORT(S)                                          AGE
neuvector-service-webui     10.100.195.99     <nodes>       8443:30257/TCP                                   15m
----
====

*PKS Change*

[NOTE]
====
PKS est testé sur le terrain et nécessite l'activation de conteneurs privilégiés sur le plan/carreau, et la modification du chemin d'accès yaml comme suit pour Allinone, Controller, Enforcer :

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====

*Taches et tolérances du nœud maître*

Toutes les informations sur les taches doivent correspondre pour programmer les Enforcers sur les nœuds. Pour vérifier les informations de taint sur un noeud (par exemple, le noeud principal) :

[,shell]
----
kubectl get node taintnodename -o yaml
----

Exemple de sortie :

[,yaml]
----
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue
----

S'il y a des taches supplémentaires comme ci-dessus, ajoutez-les à la section des tolérances de l'échantillon yaml :

[,yaml]
----
spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue
----

== Utilisation des étiquettes de nœuds pour les nœuds gestionnaires et contrôleurs

Pour contrôler les nœuds sur lesquels le gestionnaire et le contrôleur sont déployés, étiqueter chaque nœud. Remplacer nodename par le nom de nœud approprié (&apos;`+kubectl get nodes+`&apos;). Remarque : Par défaut, Kubernetes ne planifie pas les pods sur le nœud maître.

[,shell]
----
kubectl label nodes nodename nvcontroller=true
----

Ajoutez ensuite un nodeSelector au fichier yaml pour les sections de déploiement du gestionnaire et du contrôleur. Par exemple :

[,yaml]
----
          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always
----

Pour empêcher l'enforcer d'être déployé sur un nœud de contrôleur, s'il s'agit d'un nœud de gestion dédié (sans conteneurs d'applications à surveiller), ajoutez une nodeAffinity à la section yaml de l'enforcer. Par exemple :

[,yaml]
----
  app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:
----

== Rolling Updates

Les outils d'orchestration tels que Kubernetes, RedHat OpenShift et Rancher prennent en charge les mises à jour en continu avec des politiques configurables. Vous pouvez utiliser cette fonction pour mettre à jour les conteneurs {product-name}. Le plus important est de s'assurer qu'il y a au moins un contrôleur (ou Allinone) en cours d'exécution afin que les politiques, les journaux et les données de connexion ne soient pas perdus. Veillez à ce qu'il y ait un minimum de 120 secondes entre les mises à jour des conteneurs afin qu'un nouveau chef puisse être élu et que les données soient synchronisées entre les contrôleurs.

L'exemple de déploiement fourni configure déjà la politique de mise à jour continue. Si vous effectuez la mise à jour via le tableau {product-name} Helm, veuillez utiliser le tableau le plus récent pour configurer correctement les nouvelles fonctionnalités telles que le contrôle d'admission, et supprimer l'ancien rôle de cluster et la liaison de rôle de cluster pour {product-name}. Si vous effectuez la mise à jour via Kubernetes, vous pouvez mettre à jour manuellement vers une nouvelle version à l'aide des exemples de commandes ci-dessous.

=== Exemple de mise à jour continue de Kubernetes

Pour les mises à niveau qui ne nécessitent qu'une mise à jour vers une nouvelle version d'image, vous pouvez utiliser cette approche simple.

Si votre Deployment ou Daemonset est déjà en cours d'exécution, vous pouvez modifier le fichier yaml pour le remplacer par la nouvelle version, puis appliquer la mise à jour :

[,shell]
----
kubectl apply -f <yaml file>
----

Pour mettre à jour une nouvelle version de {product-name} à partir de la ligne de commande.

Pour le contrôleur en tant que déploiement (également pour le gestionnaire)

[,shell]
----
kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:<version> -n neuvector
----

Pour tout conteneur en tant que DaemonSet :

[,shell]
----
kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:<version>
----

Pour vérifier l'état de la mise à jour continue :

[,shell]
----
kubectl rollout status -n neuvector ds/neuvector-enforcer-pod
kubectl rollout status -n neuvector deployment/neuvector-controller-pod
----

Pour annuler la mise à jour :

[,shell]
----
kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod
kubectl rollout undo -n neuvector deployment/neuvector-controller-pod
----

== Exposer l'API REST dans Kubernetes

Pour exposer l'API REST à un accès depuis l'extérieur du cluster Kubernetes, voici un exemple de fichier yaml :

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-rest
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Veuillez consulter la section Automatisation pour plus d'informations sur l'API REST.

== Déploiement de Kubernetes en mode non privilégié

Les instructions suivantes peuvent être utilisées pour déployer {product-name} sans utiliser de conteneurs en mode privilégié. Le contrôleur est déjà en mode non privilégié et le déploiement de l'exécuteur doit être modifié, comme le montrent les extraits ci-dessous.

Enforcer :

[,yaml]
----
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
        # this line is required to be added if k8s version is pre-v1.19
        # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
----

== Kubernetes Deployment YAML pour v5.4.2 et plus

L'exemple YAML suivant est pour les versions 5.4.2 et suivantes où nous devons monter les certificats internes sur les pods Controller, Enforcer et Scanner puisque nous ne supportons plus les certificats codés en dur. Créez le secret du certificat interne à partir du lien donné avant de le déployer : xref:internal.adoc[Remplacement des certificats internes].

.Cliquez ici pour plus d'informations
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            privileged: true
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
          volumeMounts:
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      restartPolicy: Always
      volumes:
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert
---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`+cat /var/run/secrets/kubernetes.io/serviceaccount/token+`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"&apos;`+date +%Y-%m-%dT%H:%M:%S%z+`&apos;"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

L'exemple suivant est une référence complète de déploiement (Kubernetes 1.19+).

.Cliquez ici pour plus d'informations
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - TOKEN=`+cat /var/run/secrets/kubernetes.io/serviceaccount/token+`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"&apos;`+date +%Y-%m-%dT%H:%M:%S%z+`&apos;"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

== PKS Change

[NOTE]
====
PKS est testé sur le terrain et nécessite l'activation de conteneurs privilégiés sur le plan/carreau, et la modification du chemin d'accès yaml comme suit pour Allinone, Enforcer :

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====
