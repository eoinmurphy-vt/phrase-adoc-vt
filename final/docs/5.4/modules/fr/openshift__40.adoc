= RedHat OpenShift
:revdate: 2025-06-05
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/04.openshift/04.openshift.md
:page-opendocs-slug: /deploying/openshift

== Déployer des composants {product-name} séparés avec RedHat OpenShift

{product-name} est compatible avec les plug-ins SDN ovs standard ainsi qu'avec d'autres plug-ins tels que flannel, weave ou calico. Les exemples ci-dessous supposent l'utilisation d'un plug-in ovs standard. Cela suppose également qu'un registre docker local sera utilisé (voir les instructions à la fin pour créer le secret pour tirer dynamiquement depuis neuvector ou Docker Hub).

{product-name} https://github.com/neuvector/neuvector-helm. prend en charge le déploiement basé sur Helm avec une à l'adresse https://github.com/neuvector/neuvector-helm[carte Helm]. L'opérateur {product-name} peut également être utilisé pour le déploiement et est basé sur la carte Helm. Pour déployer les dernières versions du conteneur {product-name} à l'aide d'un opérateur, veuillez utiliser soit l'opérateur certifié Red Hat de Operator Hub, soit l'opérateur communautaire, comme indiqué dans la xref:operators.adoc[section Opérateur].

Pour un déploiement manuel, il faut d'abord extraire les conteneurs {product-name} appropriés du registre {product-name} et les placer dans votre registre local. Remarque : l'image du scanner doit être extraite régulièrement pour les mises à jour de la base de données CVE à partir de {product-name}.

=== {product-name} Images sur Docker Hub

Les images se trouvent sur le registre Docker Hub de {product-name}. Utilisez la balise de version appropriée pour le gestionnaire, le contrôleur, l'exécuteur, et laissez la version "dernière" pour l'analyseur et l'actualisateur. Par exemple :

* neuvector/manager:5.4.3
* neuvector/controller:5.4.3
* neuvector/enforcer:5.4.3
* neuvector/scanner:latest
* neuvector/updater:latest

Veillez à mettre à jour les références des images dans les fichiers yaml appropriés.

Si vous déployez avec la charte actuelle de {product-name} Helm (v1.8.9+), les modifications suivantes doivent être apportées au fichier values.yml :

* Mettre à jour le registre vers docker.io
* Mettre à jour les noms/étiquettes de l'image avec la version actuelle sur le hub Docker, comme indiqué ci-dessus.
* Laisser le champ imagePullSecrets vide

== Déployer sur OpenShift

[,shell]
----
docker login docker.io
docker pull docker.io/neuvector/manager:<version>
docker pull docker.io/neuvector/controller:<version>
docker pull docker.io/neuvector/enforcer:<version>
docker pull docker.io/neuvector/scanner
docker pull docker.io/neuvector/updater
docker logout docker.io
----

Le fichier d'exemple ci-dessous déploie un gestionnaire, 3 contrôleurs et 2 pods de numérisation. Il déploiera un enforcer sur chaque nœud en tant que daemonset, y compris sur le nœud maître (s'il est planifiable). Voir la section inférieure pour spécifier des nœuds de gestionnaires ou de contrôleurs dédiés à l'aide d'étiquettes de nœuds. Remarque : Il n'est pas recommandé de déployer (mettre à l'échelle) plus d'un gestionnaire derrière un équilibreur de charge en raison des problèmes potentiels liés à l'état de la session. Si vous prévoyez d'utiliser une revendication PersistentVolume pour stocker la sauvegarde des fichiers de configuration de {product-name}, veuillez consulter la section générale Sauvegarde/Données persistantes dans l'aperçu du xref:production.adoc#_backups_and_persistent_data[déploiement de la production].

Ensuite, définissez l'itinéraire et autorisez les conteneurs privilégiés {product-name} en suivant les instructions ci-dessous. Par défaut, OpenShift n'autorise pas les conteneurs privilégiés. De plus, par défaut, OpenShift ne planifie pas les pods sur le nœud maître. Voir les instructions à la fin pour activer/désactiver cela.

[NOTE]
====
Veuillez consulter la section Intégration d'entreprise pour plus de détails sur l'intégration avec les contrôles d'accès basés sur les rôles (RBAC) d'OpenShift.
====

. Se connecter en tant qu'utilisateur normal
+
--
[,shell]
----
oc login -u <user_name>
----
--
. Créer un nouveau projet.
+
--
[NOTE]
====
Si l'argument --node-selector est utilisé lors de la création d'un projet, cela limitera le placement des pods, comme pour le {product-name} enforcer, à des nœuds spécifiques.
====

[,shell]
----
oc new-project neuvector
----
--
. Pousser les images {product-name} vers le registre docker d'OpenShift.
+
--
[NOTE]
====
Pour OpenShift 4.6+, remplacez docker-registry.default.svc ci-dessous par image-registry.openshift-image-registry.svc dans les commandes suivantes
====

[,shell]
----
docker login -u <user_name> -p `oc whoami -t` docker-registry.default.svc:5000
docker tag docker.io/neuvector/enforcer:<version> docker-registry.default.svc:5000/neuvector/enforcer:<version>
docker tag docker.io/neuvector/controller:<version> docker-registry.default.svc:5000/neuvector/controller:<version>
docker tag docker.io/neuvector/manager:<version> docker-registry.default.svc:5000/neuvector/manager:<version>
docker tag docker.io/neuvector/scanner docker-registry.default.svc:5000/neuvector/scanner
docker tag docker.io/neuvector/updater docker-registry.default.svc:5000/neuvector/updater
docker push docker-registry.default.svc:5000/neuvector/enforcer:<version>
docker push docker-registry.default.svc:5000/neuvector/controller:<version>
docker push docker-registry.default.svc:5000/neuvector/manager:<version>
docker push docker-registry.default.svc:5000/neuvector/scanner
docker push docker-registry.default.svc:5000/neuvector/updater
docker logout docker-registry.default.svc:5000
----

[NOTE]
====
Veuillez consulter la section Mise à jour de la base de données CVE ci-dessous pour obtenir des recommandations sur la mise à jour de la dernière image du scanner dans votre registre.
====
--
. Se connecter en tant que compte system:admin
+
--
[,shell]
----
oc login -u system:admin
----
--
. Créer des comptes de service et accorder l'accès au SCC privilégié
+
--
[,shell]
----
oc create sa controller -n neuvector
oc create sa enforcer -n neuvector
oc create sa basic -n neuvector
oc create sa updater -n neuvector
oc create sa scanner -n neuvector
oc create sa registry-adapter -n neuvector
oc create sa cert-upgrader -n neuvector
oc -n neuvector adm policy add-scc-to-user privileged -z enforcer
----

Les informations suivantes seront ajoutées dans les utilisateurs privilégiés du CSC :

[,yaml]
----
- system:serviceaccount:neuvector:enforcer
----

Ajouter un nouveau scc neuvector-scc-controller pour le compte de service controller dans Openshift, en créant un fichier avec :

[,yaml]
----
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups: []
kind: SecurityContextConstraints
metadata:
  name: neuvector-scc-controller
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- ALL
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- azureFile
- projected
- secret
----

Appliquer ensuite

[,shell]
----
oc apply -f (filename)
----

Exécutez ensuite la commande suivante pour lier le compte de service du contrôleur à neuvector-scc-controller scc

[,shell]
----
oc -n neuvector adm policy add-scc-to-user neuvector-scc-controller -z controller
----

Dans OpenShift 4.6+, utilisez ce qui suit pour vérifier :

[,shell]
----
oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide
----

[,shell]
----
NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS
system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/enforcer
----

Exécutez cette commande pour vérifier le service {product-name} pour le contrôleur :

[,shell]
----
oc get rolebinding system:openshift:scc:neuvector-scc-controller -n neuvector -o wide
----

Le résultat sera le suivant

[,shell]
----
NAME                                            ROLE                                                        AGE     USERS   GROUPS   SERVICEACCOUNTS
System:openshift:scc:neuvector-scc-controller   ClusterRole/system:openshift:scc:neuvector-scc-controller   9m22s                    neuvector/controller
----
--
. Créer les ressources personnalisées (CRD) pour les règles de sécurité de {product-name}. Pour OpenShift 4.6+ (Kubernetes 1.19+) :
+
--
[,shell]
----
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
----
--
. Ajouter l'autorisation de lecture pour accéder à l'API kubernetes et aux RBAC OpenShift. 
+
[IMPORTANT]
====
Le déploiement standard de {product-name} 5.2+ utilise les comptes de service les moins privilégiés au lieu des comptes par défaut. Voir ci-dessous si vous passez à la version 5.2+ à partir d'une version antérieure à la 5.2.
====
+
--
[CAUTION]
========
Si vous passez à la version 5.3.0+, exécutez les commandes suivantes en fonction de votre version actuelle :

[tabs]
======
Version 5.2.0::
+
====
[,shell]
----
oc delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules
----
==== 

Versions antérieures à 5.2.0::
+
====
[,shell]
----
oc delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co oc delete rolebinding neuvector-admin -n neuvector
----
====
======
========

[,shell]
----
oc create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
oc create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,imagestreams.image.openshift.io
oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
oc create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
oc create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
oc create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view
oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller
oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-co --verb=get,list --resource=clusteroperators
oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller
oc create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
oc adm policy add-role-to-user neuvector-binding-secret system:serviceaccount:neuvector:controller system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:scanner system:serviceaccount:neuvector:registry-adapter -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
oc create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
oc create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
oc create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
oc create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
oc adm policy add-role-to-user neuvector-binding-cert-upgrader system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc adm policy add-role-to-user neuvector-binding-job-creation system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc adm policy add-role-to-user neuvector-binding-lease system:serviceaccount:neuvector:controller system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-nvgroupdefinitions --verb=get,list,delete --resource=nvgroupdefinitions
oc create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
----
--
. Exécutez la commande suivante pour vérifier si les comptes de service neuvector/controller, neuvector/enforcer et neuvector/updater ont été ajoutés avec succès.
+
--
[,shell]
----
oc get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co -o wide
----

Exemple de sortie :

[,shell]
----
NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               56d                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              34d                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         72d                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          72d                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   72d                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                72d                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                72d                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   72d                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                72d                    neuvector/controller
neuvector-binding-co                                ClusterRole/neuvector-binding-co                                72d                    neuvector/enforcer, neuvector/controller
----

Et ce commandement :

[,shell]
----
oc get RoleBinding neuvector-binding-scanner neuvector-binding-cert-upgrader neuvector-binding-job-creation neuvector-binding-lease neuvector-binding-secret -n neuvector -o wide
----

Exemple de sortie :

[,shell]
----
NAME                              ROLE                                   AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         56m                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   56m                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    56m                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           56m                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          56m                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter
----
--
. *(Facultatif*) Créez le maître de la fédération et/ou les services de gestion multi-clusters à distance. Si vous prévoyez d'utiliser les fonctions de gestion multi-clusters dans {product-name}, un cluster doit avoir le service Federation Master déployé, et chaque cluster distant doit avoir le service Federation Worker. Pour plus de souplesse, vous pouvez choisir de déployer les services Maître et Travailleur sur chaque cluster, de sorte que n'importe quel cluster puisse être maître ou distant.
+
--
Federated Management Services

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod
----

Créez ensuite le(s) service(s) approprié(s) :

[,shell]
----
oc create -f nv_master_worker.yaml
----
--
. Créez les services et les pods neuvector en vous basant sur les exemples de yamls ci-dessous. 
+
[IMPORTANT] 
====
Remplacez les balises <version> pour les références d'images du gestionnaire, du contrôleur et de l'exécuteur dans le fichier yaml. Apportez également toutes les autres modifications nécessaires à votre environnement de déploiement.
====
+
--
[,shell]
----
oc create -f <compose file>
----
--

C'est tout ! Vous devriez pouvoir vous connecter à la console {product-name} et vous connecter avec admin:admin, par exemple `https://<public-ip>:8443`

Pour savoir comment accéder à la console du service neuvector-webui :

[,shell]
----
oc get services -n neuvector
----

Si vous avez créé votre propre espace de noms au lieu d'utiliser "`neuvector`", remplacez toutes les instances de "`namespace: neuvector`" et les autres références à l'espace de noms par votre espace de noms dans les exemples de fichiers yaml ci-dessous.

*OpenShift 4.6+ avec CRI-O run-time*

Le nom de votre registre OpenShift par défaut peut avoir changé de docker-registry à openshift-image-registry. Vous devrez peut-être modifier le registre d'images pour le gestionnaire, le contrôleur et l'exécuteur dans l'exemple yaml.

[NOTE]
====
Le type NodePort est utilisé pour les services fed-master et fed-worker au lieu de LoadBalancer. Il se peut que vous deviez l'adapter à votre déploiement.
====

Si vous utilisez le temps d'exécution CRI-O, consultez cet https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-crio-oc.yaml[exemple de CRI-O].

*Taches et tolérances du nœud maître*

Toutes les informations sur les taches doivent correspondre pour programmer les Enforcers sur les nœuds. Pour vérifier les informations de taint sur un noeud (par exemple, le noeud principal) :

[,shell]
----
$ oc get node taintnodename -o yaml
----

Exemple de sortie :

[,yaml]
----
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue
----

S'il y a des taches supplémentaires comme ci-dessus, ajoutez-les à la section des tolérances de l'échantillon yaml :

[,yaml]
----
spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue
----

== Utilisation des étiquettes de nœuds pour les nœuds gestionnaires et contrôleurs

Pour contrôler les nœuds sur lesquels le gestionnaire et le contrôleur sont déployés, étiqueter chaque nœud. Remplacez `<nodename>` par le nom du nœud approprié.

[,shell]
----
oc label nodes <nodename> nvcontroller=true
----

Ajoutez ensuite un nodeSelector au fichier yaml pour les sections de déploiement du gestionnaire et du contrôleur. Par exemple :

[,yaml]
----
          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always
----

Pour empêcher l'enforcer d'être déployé sur un nœud de contrôleur, s'il s'agit d'un nœud de gestion dédié (sans conteneurs d'applications à surveiller), ajoutez une nodeAffinity à la section yaml de l'enforcer. Par exemple :

[,yaml]
----
app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:
----

== Mise à jour de la base de données CVE sur les déploiements OpenShift

La dernière image du scanner contient toujours la dernière mise à jour de la base de données CVE de {product-name}. C'est pourquoi il n'est pas recommandé d'utiliser une balise de version lors de l'extraction de l'image. Cependant, la mise à jour de la base de données CVE nécessite l'extraction régulière de la dernière image du scanner afin que la tâche cron de mise à jour puisse redéployer le(s) scanner(s).  Les exemples ci-dessus supposent que les images {product-name} sont extraites, étiquetées et poussées vers un registre OpenShift local. Le déploiement se fait alors à partir de ce registre plutôt que directement à partir de neuvector (ou de l'ancien registre {product-name} sur docker hub).

Pour mettre à jour régulièrement la base de données CVE, nous recommandons de créer un script/cron pour extraire la dernière image du scanner {product-name} et effectuer les étapes de marquage et de transfert dans le registre local. Cela permet de s'assurer que la base de données CVE est régulièrement mise à jour et que les images et les conteneurs sont analysés à la recherche de nouvelles vulnérabilités.

== Rolling Updates

Les outils d'orchestration tels que Kubernetes, RedHat OpenShift et Rancher prennent en charge les mises à jour en continu avec des politiques configurables. Vous pouvez utiliser cette fonction pour mettre à jour les conteneurs {product-name}. Le plus important est de s'assurer qu'il y a au moins un Allinone/Controller en fonctionnement afin que les politiques, les journaux et les données de connexion ne soient pas perdus. Veillez à ce qu'il y ait un minimum de 30 secondes entre les mises à jour des conteneurs afin qu'un nouveau chef puisse être élu et que les données soient synchronisées entre les contrôleurs.

Avant de commencer les mises à jour, veuillez retirer et étiqueter les conteneurs {product-name} de la même manière qu'au début de cette page. Vous pouvez obtenir la dernière version sans numéro de version, mais pour déclencher la mise à jour en continu, vous devez marquer l'image avec une version.

Par exemple, pour le contrôleur (le plus récent) :

[,shell]
----
docker pull neuvector/controller
----

Ensuite, pour marquer/pousser, si la dernière version est la 2.0.1, il faut procéder comme à l'étape 3 en haut de cette page :

[,shell]
----
docker login -u <user_name> -p `oc whoami -t` docker-registry.default.svc:5000
docker tag neuvector/controller docker-registry.default.svc:5000/neuvector/controller:2.0.1
docker push docker-registry.default.svc:5000/neuvector/controller:2.0.1
----

Vous pouvez maintenant mettre à jour votre fichier yaml avec ces nouvelles versions et '`apply`', ou utiliser la commande '`oc set image ...`' pour déclencher la mise à jour continue. Veuillez consulter les exemples de mise à jour continue de Kubernetes dans cette section Production pour savoir comment lancer et surveiller les mises à jour continues des conteneurs {product-name}.

L'exemple de déploiement fourni configure déjà la politique de mise à jour continue. Si vous effectuez la mise à jour via le tableau {product-name} Helm, veuillez utiliser le tableau le plus récent pour configurer correctement les nouvelles fonctionnalités telles que le contrôle d'admission, et supprimer l'ancien rôle de cluster et la liaison de rôle de cluster pour {product-name}.

== Activation de l'API REST

Pour activer l'API de repos, le port 10443 doit être configuré comme suit :

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod
----

== Activer/désactiver la programmation sur le nœud maître

Les commandes suivantes peuvent être utilisées pour activer/désactiver la planification sur le nœud maître.

[,shell]
----
oc adm manage-node nodename --schedulable
----

[,shell]
----
oc adm manage-node nodename --schedulable=false
----

== Déploiement d'OpenShift en mode non privilégié

Les instructions suivantes peuvent être utilisées pour déployer {product-name} sans utiliser de conteneurs en mode privilégié. Le contrôleur est déjà en mode non privilégié et le déploiement de l'exécuteur doit être modifié, comme le montrent les extraits ci-dessous.

Enforcer :

[,yaml]
----
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # this line below is required to be added if k8s version is pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # openshift
            seLinuxOptions:
              type: unconfined_t
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
----

L'exemple suivant est une référence complète de déploiement utilisant le temps d'exécution cri-o. Pour d'autres exécutions, veuillez apporter les modifications appropriées aux volumes/montages de volumes pour crio.sock.

.Cliquez ici pour plus d'informations
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: ClusterIP
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: neuvector-route-webui
  namespace: neuvector
spec:
  to:
    kind: Service
    name: neuvector-service-webui
  port:
    targetPort: manager
  tls:
    termination: passthrough

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/manager:<version>
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/controller:<version>
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # - name: CTRL_PERSIST_CONFIG
            #   value: "1"
          volumeMounts:
            # - mountPath: /var/neuvector
            #   name: nv-share
            #   readOnly: false
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        # - name: nv-share
        #   persistentVolumeClaim:
        #     claimName: neuvector-data
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/enforcer:<version>
          securityContext:
            # openshift
            seLinuxOptions:
              type: unconfined_t
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            # - mountPath: /run/runtime.sock
            #   name: runtime-sock
            #   readOnly: true
            # - mountPath: /host/proc
            #   name: proc-vol
            #   readOnly: true
            # - mountPath: /host/cgroup
            #   name: cgroup-vol
            #   readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        # - name: runtime-sock
        #   hostPath:
        #     path: /var/run/crio/crio.sock
        # - name: proc-vol
        #   hostPath:
        #     path: /proc
        # - name: cgroup-vol
        #   hostPath:
        #     path: /sys/fs/cgroup
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/scanner:<version>
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: image-registry.openshift-image-registry.svc:5000/neuvector/updater:<version>
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====
