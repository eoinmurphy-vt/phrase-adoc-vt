= Bereitstellung von {product-name}
:revdate: 2024-09-27
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/01.production/01.production.md
:page-opendocs-slug: /Entwicklung/Produktion

== Planung von Einsätzen

Zu den {product-name} Containern in einer Standardbereitstellung gehören der Controller, der Manager, der Enforcer, der Scanner und der Updater. Es muss überlegt werden, wo diese Container (auf welchen Knoten) eingesetzt werden, und es müssen geeignete Kennzeichnungen, Taints oder Toleranzen geschaffen werden, um sie zu kontrollieren.

Der Enforcer sollte auf jedem Host/Knoten installiert werden, auf dem Anwendungscontainer laufen, die von {product-name} überwacht und geschützt werden sollen.

Der Controller verwaltet den Enforcer-Cluster und kann auf demselben Knoten wie ein Enforcer oder auf einem separaten Verwaltungsknoten bereitgestellt werden. Der Manager sollte auf dem Knoten installiert werden, auf dem der Controller läuft, und bietet Konsolenzugriff auf den Controller. Andere erforderliche {product-name} Container wie der Manager, der Scanner und der Updater werden in dem unten genannten Leitfaden für bewährte Praktiken ausführlicher beschrieben.

Wenn Sie dies noch nicht getan haben, ziehen Sie die Images aus dem {product-name} Docker Hub.

Die Images befinden sich in der {product-name} Docker Hub Registry. Verwenden Sie das entsprechende Versions-Tag für Manager, Controller und Enforcer und belassen Sie die Version für Scanner und Updater auf "latest". Zum Beispiel:

* neuvector/manager:5.3.2
* neuvector/controller:5.3.2
* neuvector/enforcer:5.3.2
* neuvector/scanner:latest
* neuvector/updater:latest

Bitte stellen Sie sicher, dass Sie die Bildreferenzen in den entsprechenden yaml-Dateien aktualisieren.

Wenn Sie mit dem aktuellen {product-name} Helm-Diagramm (v1.8.9+) bereitstellen, sollten Sie die folgenden Änderungen an values.yml vornehmen:

* Aktualisieren Sie die Registry auf docker.io
* Aktualisieren Sie die Image-Namen/Tags auf die aktuelle Version auf Docker Hub, wie oben gezeigt
* Lassen Sie das Feld imagePullSecrets leer

=== Bewährte Praktiken, Tipps, Fragen und Antworten zur Bereitstellung und Verwaltung {product-name}

Laden Sie das xref:attachment$NV_Onboarding_5.0.pdf[Dokument Best Practices für die Bereitstellung] herunter und lesen Sie es durch. Es enthält Tipps zu Leistung und Dimensionierung, Best Practices und häufig gestellte Fragen zu Bereitstellungen.

== Einsatz mit Helm oder Operators

Die automatisierte Bereitstellung mit Helm finden Sie unter https://github.com/neuvector/neuvector-helm.

Die Bereitstellung mit einem Operator, einschließlich RedHat Certified Operator und Kubernetes Community Operator, wird unterstützt, eine allgemeine Beschreibung xref:operators.adoc[finden Sie hier]. Der {product-name} RedHat-Operator ist unter https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, und der Community-Operator unter https://operatorhub.io/operator/neuvector-operator.

== Bereitstellung mit ConfigMap

Die automatisierte Bereitstellung in Kubernetes wird mit einer ConfigMap unterstützt. Weitere Einzelheiten finden Sie im Abschnitt xref:configmap.adoc[Bereitstellen mit ConfigMap].

== Einsetzen der Steuergeräte

Es wird empfohlen, mehrere Controller für eine Hochverfügbarkeitskonfiguration (HA) einzusetzen. Die Lotsen verwenden das auf Konsens basierende RAFT-Protokoll, um einen Anführer zu wählen und, falls dieser ausfällt, einen anderen Anführer zu wählen. Aus diesem Grund sollte die Anzahl der aktiven Regler eine ungerade Zahl sein, z. B. 3, 5, 7 usw.

== Controller HA

Die Controller synchronisieren alle Daten untereinander, einschließlich Konfiguration, Richtlinien, Konversationen, Ereignisse und Benachrichtigungen.

Wenn der primäre aktive Kontrolleur ausfällt, wird automatisch ein neuer Anführer gewählt, der die Leitung übernimmt.

Treffen Sie besondere Vorkehrungen, um sicherzustellen, dass immer ein Controller in Betrieb und einsatzbereit ist, insbesondere bei Aktualisierungen und Neustarts des Host-Betriebssystems oder der Orchestrierungsplattform.

== Backups und dauerhafte Daten

Achten Sie darauf, dass Sie die Konfigurationsdatei regelmäßig aus der Konsole exportieren und als Backup speichern.

Wenn Sie mehrere Controller in einer HA-Konfiguration betreiben, werden alle Daten zwischen den Controllern synchronisiert, solange ein Controller immer in Betrieb ist.

Wenn Sie Protokolle wie Verstöße, Bedrohungen, Schwachstellen und Ereignisse speichern möchten, aktivieren Sie den SYSLOG-Server in den Einstellungen.

{product-name} unterstützt persistente Daten für die Richtlinie {product-name} und die Konfiguration. Damit wird ein Echtzeit-Backup so konfiguriert, dass ein Volume unter /var/neuvector/ vom Controller-Pod eingehängt wird. Der primäre Anwendungsfall ist, wenn das persistente Volume gemountet wird, die Konfiguration und die Richtlinie während der Laufzeit auf dem persistenten Volume gespeichert werden. Im Falle eines Totalausfalls des Clusters wird die Konfiguration automatisch wiederhergestellt, wenn der neue Cluster erstellt wird. Konfiguration und Richtlinien können auch manuell wiederhergestellt oder aus dem Volume /var/neuvector/ entfernt werden.

[IMPORTANT]
====
Wenn ein persistentes Volume nicht gemountet ist, speichert {product-name} die Konfiguration oder Richtlinie NICHT als persistente Daten. Stellen Sie sicher, dass Sie die Controller-Konfiguration und die Richtlinie sichern, bevor Sie den allinone- oder Controller-Container stoppen. Dies kann unter `Settings -> Configuration` geschehen. Alternativ kann der Controller in einer HA-Konfiguration mit 3 oder 5 laufenden Controllern eingesetzt werden. In diesem Fall bleibt die Richtlinie auf den anderen Controllern bestehen, während ein Controller aktualisiert wird.
====

=== Beispiel für ein persistentes Volumen

Das im Cluster definierte PersistentVolume ist für die Unterstützung persistenter Volumes erforderlich. Die Anforderung für {product-name} ist, dass die Zugriffsmodi ReadWriteMany(RWX) sein müssen. Nicht alle Speichertypen unterstützen den RWX-Zugriffsmodus. Auf GKE müssen Sie beispielsweise ein RWX-Dauerlaufwerk mit NFS-Speicher erstellen.

Sobald das PersistentVolume erstellt ist, muss ein PersistentVolumeClaim wie unten für Controller erstellt werden. Derzeit wird das persistente Volume nur für die {product-name} Konfigurations-Backupdateien im Controller (Richtlinien, Regeln, Benutzerdaten, Integrationen usw.) und die Ergebnisse der Registrierungsprüfung verwendet.

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
----

Hier ist ein Beispiel für IBM Cloud:

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
  labels:
    billingType: "hourly"
    region: us-south
    zone: sjc03
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
      iops: "100"
  storageClassName: ibmc-file-retain-custom
----

Nachdem der Persistent Volume Claim erstellt wurde, ändern Sie die Beispiel-Yaml-Datei {product-name} wie unten gezeigt (alter Abschnitt auskommentiert):

[,yaml]
----
...
spec:
  template:
    spec:
      volumes:
        - name: nv-share
#         hostPath:                        // replaced by persistentVolumeClaim
#           path: /var/neuvector        // replaced by persistentVolumeClaim
          persistentVolumeClaim:
            claimName: neuvector-data
----

Fügen Sie außerdem die folgende Umgebungsvariable in die Controller- oder Allinone-Beispiel-Yamls ein, um persistente Volumes zu unterstützen. Dadurch liest der Controller beim Start die Backup-Konfiguration.

[,yaml]
----
            - name: CTRL_PERSIST_CONFIG
----

=== ConfigMaps und persistente Speicherung

Sowohl die ConfigMaps als auch die Sicherungskopie des persistenten Speichers werden nur gelesen, wenn ein neuer {product-name} -Cluster bereitgestellt wird oder der Cluster ausfällt und neu gestartet wird. Sie werden bei rollenden Upgrades nicht verwendet.

Die Konfigurationssicherung des persistenten Speichers wird zuerst gelesen, dann werden die ConfigMaps angewendet, so dass die ConfigMap-Einstellungen Vorrang haben. Alle ConfigMap-Einstellungen (z. B. Aktualisierungen) werden ebenfalls im permanenten Speicher abgelegt.

Weitere Informationen finden Sie im Abschnitt xref:configmap.adoc[ConfigMaps].

== Aktualisieren der CVE-Schwachstellen-Datenbank in der Produktion

Bitte lesen Sie in jedem Beispielabschnitt nach, wie Sie die CVE-Datenbank auf dem neuesten Stand halten können.

Die Version der CVE-Datenbank kann in der Konsole auf der Registerkarte Sicherheitsrisiken eingesehen werden. Sie können auch das Image des Updater-Containers überprüfen.

[,shell]
----
docker inspect neuvector/updater
----

[,json]
----
"Labels": {
                "neuvector.image": "neuvector/updater",
                "neuvector.role": "updater",
                "neuvector.vuln_db": "1.255"
            }
----

Prüfen Sie nach der Aktualisierung die Controller/Allinone-Protokolle auf "Version". Zum Beispiel in Kubernetes:

[,shell]
----
kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version

...
2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576
----

== Zugriff auf die Konsole

Standardmäßig wird die Konsole als Dienst an Port 8443 oder als nodePort mit einem zufälligen Port auf jedem Host bereitgestellt. Im ersten Abschnitt Grundlagen -> xref:rest-api.adoc[Connect to Manager] finden Sie Optionen zum Deaktivieren von HTTPS oder zum Zugriff auf die Konsole über eine Unternehmensfirewall, die den Port 8443 für den Konsolenzugriff nicht zulässt.

== Weitergabe von Host-Aktualisierungen oder automatische Skalierung von Knoten mit einem Budget für Pod-Unterbrechungen

Wartungs- oder Skalierungsaktivitäten können die Controller auf den Knoten beeinträchtigen. Public-Cloud-Anbieter unterstützen die Fähigkeit zur automatischen Skalierung von Knoten, die Pods einschließlich der {product-name} Controller dynamisch evakuieren können. Um Unterbrechungen der Steuerungen zu vermeiden, kann ein {product-name} Pod-Störungsbudget erstellt werden.

Erstellen Sie z. B. die folgende Datei nv_pdb.yaml, um sicherzustellen, dass zu jedem Zeitpunkt mindestens 2 Controller in Betrieb sind.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

Dann

[,shell]
----
kubectl create -f nv_pdb.yaml
----

Für weitere Informationen: https://kubernetes.io/docs/tasks/run-application/configure-pdb/

== Bereitstellen ohne privilegierten Modus

Auf einigen Systemen wird die Bereitstellung ohne Verwendung des privilegierten Modus unterstützt. Diese Systeme müssen die Seccom-Funktionen und die Einstellung des Scharfschaltungsprofils unterstützen.

Beispiele für Compose-Dateien finden Sie im Abschnitt über die xref:docker.adoc[Docker-Bereitstellung].

== Multi-Standort, Multi-Cluster Architektur

Für Unternehmen mit mehreren Standorten, bei denen für jeden Standort ein eigener {product-name} -Cluster eingerichtet werden kann, wird im Folgenden eine Referenzarchitektur vorgeschlagen. Jeder Cluster hat seinen eigenen Satz von Controllern und wird separat verwaltet.

image:multisite.png[Multi-Site]

Eine ausführlichere Beschreibung finden Sie in dieser Datei >xref:attachment$multisite.pdf[{product-name} Multi-Site Architecture]
