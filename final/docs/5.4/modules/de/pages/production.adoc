= Bereitstellung von {product-name}
:revdate: 2024-09-27
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/01.production/01.production.md
:page-opendocs-slug: /bereitstellung/produktion

== Planung von Bereitstellungen

Die {product-name} Container in einer Standardbereitstellung umfassen den Controller, Manager, Durchsetzer, Scanner und Aktualisierer. Die Platzierung, wo diese Container (auf welchen Knoten) bereitgestellt werden, muss berücksichtigt werden, und geeignete Labels, Taints oder Toleranzen müssen erstellt werden, um sie zu steuern.

Der Durchsetzer sollte auf jedem Host/Knoten bereitgestellt werden, auf dem Anwendungscontainer, die von {product-name} überwacht und geschützt werden sollen, ausgeführt werden.

Der Controller verwaltet den Cluster von Durchsetzern und kann auf demselben Knoten wie ein Durchsetzer oder auf einem separaten Verwaltungs-Knoten bereitgestellt werden. Der Manager sollte auf dem Knoten bereitgestellt werden, auf dem der Controller läuft, und wird Zugriff auf die Konsole des Controllers bieten. Andere erforderliche {product-name} Container wie der Manager, Scanner und Aktualisierer werden im Best Practices-Leitfaden, auf den unten verwiesen wird, ausführlicher beschrieben.

Wenn Sie dies noch nicht getan haben, ziehen Sie die Bilder vom {product-name} Docker Hub.

Die Bilder befinden sich im {product-name} Docker Hub-Registry. Verwenden Sie das entsprechende Versions-Tag für den Manager, Controller, Durchsetzer und lassen Sie die Version für Scanner und Aktualisierer als 'latest'. Zum Beispiel:

* neuvector/manager:5.3.2
* neuvector/controller:5.3.2
* neuvector/enforcer:5.3.2
* neuvector/scanner:latest
* neuvector/updater:latest

Bitte stellen Sie sicher, dass Sie die Bildreferenzen in den entsprechenden YAML-Dateien aktualisieren.

Wenn Sie mit dem aktuellen {product-name} Helm-Chart (v1.8.9+) bereitstellen, sollten die folgenden Änderungen an values.yml vorgenommen werden:

* Aktualisieren Sie das Repository auf docker.io
* Aktualisieren Sie die Bildnamen/Tags auf die aktuelle Version auf Docker Hub, wie oben gezeigt
* Lassen Sie die imagePullSecrets leer

=== Best Practices, Tipps, Fragen und Antworten für das Bereitstellen und Verwalten von {product-name}

Laden Sie dieses xref:attachment$NV_Onboarding_5.0.pdf[Dokument zu den besten Praktiken für die Bereitstellung] herunter und überprüfen Sie es auf Tipps zu Leistung und Dimensionierung, bewährte Verfahren und häufig gestellte Fragen zu Bereitstellungen.

== Bereitstellung mit Helm oder Operatoren

Automatisierte Bereitstellung mit Helm finden Sie unter https://github.com/neuvector/neuvector-helm.

Die Bereitstellung mit einem Operator, einschließlich des RedHat-zertifizierten Operators und des Kubernetes-Community-Operators, wird unterstützt, mit einer allgemeinen Beschreibung xref:operators.adoc[hier]. Der {product-name} RedHat-Operator befindet sich unter https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, und der Community-Operator unter https://operatorhub.io/operator/neuvector-operator.

== Bereitstellung mit ConfigMap

Die automatisierte Bereitstellung auf Kubernetes wird mit einer ConfigMap unterstützt. Bitte sehen Sie sich den Abschnitt xref:configmap.adoc[Bereitstellung mit ConfigMap] für weitere Details an.

== Bereitstellung der Controller

Wir empfehlen, mehrere Controller für eine Hochverfügbarkeitskonfiguration (HA) auszuführen. Die Controller verwenden das konsensbasierte RAFT-Protokoll, um einen Führer zu wählen, und wenn der Führer ausfällt, um einen anderen Führer zu wählen. Aus diesem Grund sollte die Anzahl der aktiven Controller eine ungerade Zahl sein, zum Beispiel 3, 5, 7 usw.

== Controller HA

Die Controller synchronisieren alle Daten untereinander, einschließlich Konfiguration, Richtlinie, Gespräche, Ereignisse und Benachrichtigungen.

Wenn der primäre aktive Controller ausfällt, wird automatisch ein neuer Führer gewählt und übernimmt.

Treffen Sie besondere Vorkehrungen, um sicherzustellen, dass immer ein Controller läuft und bereit ist, insbesondere während Updates und Neustarts des Host-Betriebssystems oder der Orchestrierungsplattform.

== Backups und persistente Daten

Stellen Sie sicher, dass Sie die Konfigurationsdatei regelmäßig aus der Konsole exportieren und als Backup speichern.

Wenn Sie mehrere Controller in einer HA-Konfiguration betreiben, werden alle Daten zwischen den Controllern synchronisiert, solange immer ein Controller aktiv ist.

Wenn Sie Protokolle wie Verstöße, Bedrohungen, Schwachstellen und Ereignisse speichern möchten, aktivieren Sie bitte den SYSLOG-Server in den Einstellungen.

{product-name} unterstützt persistente Daten für die {product-name}-Richtlinie und -Konfiguration. Dies konfiguriert ein Echtzeit-Backup, um ein Volume unter /var/neuvector/ vom Controller-Pod zu mounten. Der Hauptanwendungsfall ist, wenn das persistente Volume gemountet ist, dass die Konfiguration und Richtlinie zur Laufzeit im persistenten Volume gespeichert werden. Im Falle eines totalen Ausfalls des Clusters wird die Konfiguration automatisch wiederhergestellt, wenn der neue Cluster erstellt wird. Konfiguration und Richtlinie können auch manuell aus dem /var/neuvector/-Volume wiederhergestellt oder entfernt werden.

[IMPORTANT]
====
Wenn ein persistentes Volume nicht gemountet ist, speichert {product-name} die Konfiguration oder Richtlinie NICHT als persistente Daten. Stellen Sie sicher, dass Sie die Konfiguration und Richtlinie des Controllers sichern, bevor Sie den All-in-One- oder Controller-Container stoppen. Dies kann in `Settings -> Configuration` erfolgen. Alternativ kann der Controller in einer HA-Konfiguration mit 3 oder 5 laufenden Controllern bereitgestellt werden, wobei die Richtlinie mit anderen Controllern bestehen bleibt, während einer aktualisiert wird.
====

=== Beispiel für persistentes Volume

Das im Cluster definierte PersistentVolume ist für die Unterstützung von persistenten Volumes erforderlich. Die Anforderung für {product-name} ist, dass die accessModes ReadWriteMany (RWX) sein müssen. Nicht alle Speichertypen unterstützen den RWX-Zugriffsmodus. Zum Beispiel müssen Sie auf GKE möglicherweise ein RWX-persistentes Volume mit NFS-Speicher erstellen.

Sobald das PersistentVolume erstellt ist, muss ein PersistentVolumeClaim wie unten für den Controller erstellt werden. Der persistente Speicher wird derzeit nur für die {product-name} Konfigurationssicherungsdateien im Controller (Richtlinien, Regeln, Benutzerdaten, Integrationen usw.) und die Ergebnisse des Registry-Scans verwendet.

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
----

Hier ist ein Beispiel für IBM Cloud:

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
  labels:
    billingType: "hourly"
    region: us-south
    zone: sjc03
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
      iops: "100"
  storageClassName: ibmc-file-retain-custom
----

Nachdem der Persistent Volume Claim erstellt wurde, ändern Sie die {product-name} Beispiel-YAML-Datei wie unten gezeigt (alter Abschnitt auskommentiert):

[,yaml]
----
...
spec:
  template:
    spec:
      volumes:
        - name: nv-share
#         hostPath:                        // replaced by persistentVolumeClaim
#           path: /var/neuvector        // replaced by persistentVolumeClaim
          persistentVolumeClaim:
            claimName: neuvector-data
----

Fügen Sie auch die folgende Umgebungsvariable in den Controller- oder All-in-One-Beispiel-YAMLs für die Unterstützung des persistenten Speichers hinzu. Dies wird den Controller dazu bringen, die Sicherungskonfiguration beim Start zu lesen.

[,yaml]
----
            - name: CTRL_PERSIST_CONFIG
----

=== ConfigMaps und Persistenter Speicher

Sowohl die ConfigMaps als auch die Sicherung des persistenten Speichers werden nur gelesen, wenn ein neuer {product-name} Cluster bereitgestellt wird oder der Cluster ausfällt und neu gestartet wird. Sie werden während der Rolling-Upgrades nicht verwendet.

Die Sicherung der Konfiguration des persistenten Speichers wird zuerst gelesen, dann werden die ConfigMaps angewendet, sodass die Einstellungen der ConfigMap Vorrang haben. Alle Einstellungen der ConfigMap (z. B. Updates) werden ebenfalls im persistenten Speicher gespeichert.

Für weitere Informationen siehe den Abschnitt xref:configmap.adoc[ConfigMaps].

== Aktualisierung der CVE-Sicherheitsdatenbank in der Produktion

Bitte sehen Sie sich jeden Beispielabschnitt für Anweisungen an, wie Sie die CVE-Datenbank aktuell halten.

Die Version der CVE-Datenbank kann in der Konsole im Tab "Schwachstellen" angezeigt werden. Sie können auch das Updater-Container-Image inspizieren.

[,shell]
----
docker inspect neuvector/updater
----

[,json]
----
"Labels": {
                "neuvector.image": "neuvector/updater",
                "neuvector.role": "updater",
                "neuvector.vuln_db": "1.255"
            }
----

Nach dem Ausführen des Updates, überprüfen Sie die Protokolle des Controllers/All-in-One auf 'Version.' Zum Beispiel in Kubernetes:

[,shell]
----
kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version

...
2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576
----

== Zugriff auf die Konsole

Standardmäßig wird die Konsole als Dienst auf Port 8443 oder als NodePort mit einem zufälligen Port auf jedem Host bereitgestellt. Bitte sehen Sie sich den ersten Abschnitt Grundlagen -> xref:rest-api.adoc[Mit Manager verbinden] für Optionen an, um HTTPS zu deaktivieren oder auf die Konsole über eine Unternehmensfirewall zuzugreifen, die den Port 8443 für den Konsolenzugriff nicht zulässt.

== Verwaltung von Host-Updates oder Auto-Scaling-Knoten mit einem Pod-Störungshaushalt

Wartungs- oder Skalierungsaktivitäten können die Controller auf Knoten beeinträchtigen. Öffentliche Cloud-Anbieter unterstützen die Möglichkeit, Knoten automatisch zu skalieren, was das dynamische Entfernen von Pods, einschließlich der {product-name} Controller, ermöglichen kann. Um Störungen der Controller zu verhindern, kann ein {product-name} Pod-Störungshaushalt erstellt werden.

Zum Beispiel erstellen Sie die folgende Datei nv_pdb.yaml, um sicherzustellen, dass jederzeit mindestens 2 Controller ausgeführt werden.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

Dann

[,shell]
----
kubectl create -f nv_pdb.yaml
----

Für weitere Details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/

== Bereitstellung ohne privilegierten Modus

Auf einigen Systemen wird die Bereitstellung ohne Verwendung des privilegierten Modus unterstützt. Diese Systeme müssen seccom-Funktionen und das Setzen des AppArmor-Profils unterstützen.

Siehe den Abschnitt über xref:docker.adoc[Docker-Bereitstellung] für Beispiel-Compose-Dateien.

== Multi-Site, Multi-Cluster-Architektur

Für Unternehmen mit mehreren Standorten, an denen ein separater {product-name} Cluster für jeden Standort bereitgestellt werden kann, wird die folgende Referenzarchitektur vorgeschlagen. Jeder Cluster hat sein eigenes Set von Controllern und wird separat verwaltet.

image:multisite.png[Multi-Site]

Siehe eine detailliertere Beschreibung in dieser Datei >
xref:attachment$multisite.pdf[{product-name} Multi-Site-Architektur]
