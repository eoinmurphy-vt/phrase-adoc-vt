= Kubernetes
:revdate: 2025-06-05
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/02.kubernetes/02.kubernetes.md
:page-opendocs-slug: /deploying/kubernetes

== Bereitstellung mit Kubernetes

Sie können Kubernetes verwenden, um separate Manager-, Controller- und Enforcer-Container bereitzustellen und sicherzustellen, dass auf allen neuen Knoten ein Enforcer bereitgestellt wird. {product-name} erfordert und unterstützt Kubernetes-Netzwerk-Plugins wie Flannel, Weave oder Calico.

Die Beispieldatei wird einen Manager und 3 Controller einsetzen. Es wird ein Enforcer auf jedem Knoten als Daemonset bereitgestellt. Standardmäßig wird das folgende Beispiel auch auf dem Master-Knoten bereitgestellt.

Siehe den unteren Abschnitt zur Angabe von dedizierten Manager- oder Controllerknoten mit Hilfe von Knotenbeschriftungen. 

[NOTE]
====
Es wird nicht empfohlen, mehr als einen Manager hinter einem Load Balancer einzusetzen (zu skalieren), da es zu Problemen mit dem Sitzungsstatus kommen kann. Wenn Sie planen, einen PersistentVolume-Anspruch zu verwenden, um das Backup der {product-name} Konfigurationsdateien zu speichern, lesen Sie bitte den allgemeinen Abschnitt Backup/Persistente Daten in der Übersicht über xref:production.adoc#_backups_and_persistent_data[die Bereitstellung von {product-name}].
====

Wenn Ihr Einsatz einen integrierten Load Balancer unterstützt, ändern Sie den Typ NodePort in LoadBalancer für die Konsole in der yaml-Datei unten.

{product-name} unterstützt den Helm-basierten Einsatz mit einem Helm-Diagramm unter https://github.com/neuvector/neuvector-helm.

Es gibt einen separaten Abschnitt für OpenShift-Anweisungen, und für Docker EE auf Kubernetes gibt es einige spezielle Schritte, die im Abschnitt über Docker beschrieben werden.

=== {product-name} Bilder auf Docker Hub

Die Images befinden sich in der {product-name} Docker Hub Registry. Verwenden Sie das entsprechende Versions-Tag für Manager, Controller und Enforcer und belassen Sie die Version für Scanner und Updater auf "latest". Zum Beispiel:

* neuvector/manager:5.4.3
* neuvector/controller:5.4.3
* neuvector/enforcer:5.4.3
* neuvector/scanner:latest
* neuvector/updater:latest

Bitte stellen Sie sicher, dass Sie die Bildreferenzen in den entsprechenden yaml-Dateien aktualisieren.

Wenn Sie mit dem aktuellen {product-name} Helm-Diagramm (v1.8.9+) bereitstellen, sollten Sie die folgenden Änderungen an values.yml vornehmen:

* Aktualisieren Sie die Registry auf docker.io
* Aktualisieren Sie die Image-Namen/Tags auf die aktuelle Version im Docker-Hub, wie oben gezeigt
* Lassen Sie das Feld imagePullSecrets leer

[NOTE]
====
Bei der Bereitstellung aus dem Rancher Manager 2.6.5+ {product-name} werden die Images automatisch aus dem gespiegelten Rancher Registry-Image-Repository gezogen und im cattle-neuvector-system-Namensraum bereitgestellt.
====


== Bereitstellung von {product-name}

. Erstellen Sie den Namensraum {product-name} und die erforderlichen Dienstkonten: 
+
--
[,shell]
----
kubectl create namespace neuvector
kubectl create sa controller -n neuvector
kubectl create sa enforcer -n neuvector
kubectl create sa basic -n neuvector
kubectl create sa updater -n neuvector
kubectl create sa scanner -n neuvector
kubectl create sa registry-adapter -n neuvector
kubectl create sa cert-upgrader -n neuvector
----
--
. *(Optional*) Erstellen Sie die {product-name} Pod Security Admission (PSA) oder Pod Security Policy (PSP). Wenn Sie Pod Security Admission (auch bekannt als Pod Security Standards) in Kubernetes 1.25+ oder Pod Security Policies (vor 1.25) in Ihrem Kubernetes-Cluster aktiviert haben, fügen Sie Folgendes für {product-name} hinzu (z. B. nv_psp.yaml). 
+
[NOTE]
====
* PSP ist in Kubernetes 1.21 veraltet und wird in 1.25 vollständig entfernt werden. 
* Die Manager- und Scanner-Pods werden ohne eine uid ausgeführt. Wenn Ihr PSP über eine Regel `Run As User: Rule: MustRunAsNonRoot` verfügt, fügen Sie Folgendes in die unten stehende Beispiel-Yaml ein (mit dem entsprechenden Wert für `###`):
====
+
--
[,yaml]
----
securityContext:
    runAsUser: ###
----

Für PSA in Kubernetes 1.25+ kennzeichnen Sie den Namespace {product-name} mit einem privilegierten Profil für die Bereitstellung in einem PSA-aktivierten Cluster. 
[,shell]
----
kubectl label namespace neuvector "pod-security.kubernetes.io/enforce=privileged" 
----
--
. Erstellen Sie die benutzerdefinierten Ressourcen (CRD) für {product-name} Sicherheitsregeln. Für Kubernetes 1.19+: 
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
----
--
. Fügen Sie eine Leseberechtigung für den Zugriff auf die Kubernetes-API hinzu. 
+
[IMPORTANT]
====
Die Standardimplementierung von {product-name} 5.2+ verwendet Dienstkonten mit den geringsten Rechten anstelle der Standardkonten. Siehe unten, wenn Sie von einer Version vor 5.3 aktualisieren. 
====
+
[WARNING]
====
Wenn Sie auf 5.3.0+ aktualisieren, führen Sie die folgenden Befehle auf der Grundlage Ihrer aktuellen Version aus:
====
+
--
[tabs]
======
Version 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules 
----
====

Versionen vor 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules
kubectl delete rolebinding neuvector-admin -n neuvector 
----
====
======

Wenden Sie die Leseberechtigungen über die folgenden "create clusterrole"-Befehle an: 

[,shell]
----
kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io
kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller
kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller --serviceaccount=neuvector:enforcer --serviceaccount=neuvector:scanner --serviceaccount=neuvector:registry-adapter -n neuvector
kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller 
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
kubectl create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
kubectl create rolebinding neuvector-binding-cert-upgrader --role=neuvector-binding-cert-upgrader --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create rolebinding neuvector-binding-job-creation --role=neuvector-binding-job-creation --serviceaccount=neuvector:controller -n neuvector
kubectl create rolebinding neuvector-binding-lease --role=neuvector-binding-lease --serviceaccount=neuvector:controller --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create clusterrole neuvector-binding-nvgroupdefinitions --verb=list,get,delete --resource=nvgroupdefinitions
kubectl create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
----
--
. Führen Sie die folgenden Befehle aus, um zu überprüfen, ob die Dienstkonten neuvector/controller und neuvector/updater erfolgreich hinzugefügt wurden. 
+
--
[,shell]
----
kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvgroupdefinitions -o wide
----

Beispielhafte Ausgabe: 

[,shell]
----
NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               45s                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              45s                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         44s                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          44s                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   43s                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                43s                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                43s                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   43s                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                43s                    neuvector/controller
neuvector-binding-nvgroupdefinitions                ClusterRole/neuvector-binding-nvgroupdefinitions                40s                    neuvector/controller
----

Und dieser Befehl: 

[,shell]
----
kubectl get RoleBinding neuvector-binding-scanner neuvector-binding-cert-upgrader neuvector-binding-job-creation neuvector-binding-lease neuvector-binding-secret -n neuvector -o wide
----

Beispielhafte Ausgabe: 

[,shell]
----
NAME                              ROLE                                   AGE    USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         8m8s                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   8m8s                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    8m8s                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           8m8s                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          8m8s                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter
----
--
. *(Optional*) Erstellen Sie die Federation Master und/oder Remote Multi-Cluster Management Services. Wenn Sie die Multi-Cluster-Verwaltungsfunktionen in {product-name} verwenden möchten, muss auf einem Cluster der Federation Master-Dienst und auf jedem Remote-Cluster der Federation Worker-Dienst installiert sein. Um flexibel zu sein, können Sie auf jedem Cluster sowohl Master- als auch Worker-Dienste einrichten, so dass jeder Cluster als Master oder Remote fungieren kann. Cluster-Management im Verbund 
+
--
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Erstellen Sie dann den/die entsprechenden Dienst(e): 

[,shell]
----
kubectl create -f nv_master_worker.yaml 
----
--
. Erstellen Sie die primären {product-name} Dienste und Pods unter Verwendung der voreingestellten Versionsbefehle oder ändern Sie das unten stehende Beispiel yaml. Die voreingestellte Version ruft einen LoadBalancer für die {product-name} Console auf. Wenn Sie die unten stehende Beispiel-Yaml-Datei verwenden, ersetzen Sie die Bildnamen und <version> -Tags für die Manager-, Controller- und Enforcer-Bildreferenzen in der Yaml-Datei. Nehmen Sie auch alle anderen Änderungen vor, die für Ihre Einsatzumgebung erforderlich sind (z. B. LoadBalancer/NodePort/Ingress für den Managerzugriff usw.). Die unten stehende YAML muss für interne Zertifikatsänderungen geändert werden, wenn sie von v5.4.2 oder höher bereitgestellt wird. Siehe diese <<_kubernetes_deployment_yaml_for_v5_4_2_onwards,YAML>>.
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-k8s.yaml 
----

Oder, wenn Sie eines der oben genannten yaml oder Beispiele von unten ändern: 

[,shell]
----
kubectl create -f neuvector.yaml 
----

Das war's! Sie sollten in der Lage sein, sich mit der Konsole {product-name} zu verbinden und sich mit admin:admin anzumelden, z. B. `https://<public-ip>:8443`
--

[NOTE]
====
Der in der Datei neuvector.yaml angegebene Nodeport-Dienst öffnet auf allen Kubernetes-Knoten einen zufälligen Port für den Webkonsolen-Port der {product-name} -Verwaltung. Alternativ können Sie einen LoadBalancer oder Ingress verwenden, der eine öffentliche IP und den Standard-Port 8443 nutzt. Stellen Sie sicher, dass Sie für nodeport den Zugang über Firewalls für diesen Port öffnen, falls erforderlich. Wenn Sie sehen möchten, welcher Port auf den Host-Knoten offen ist, führen Sie bitte die folgenden Befehle aus:

[,shell]
----
kubectl get svc -n neuvector
----

Und Sie werden etwas sehen wie:

[,shell]
----
NAME                          CLUSTER-IP      EXTERNAL-IP   PORT(S)                                          AGE
neuvector-service-webui     10.100.195.99     <nodes>       8443:30257/TCP                                   15m
----
====

*PKS Veränderung*

[NOTE]
====
PKS ist feldgetestet und erfordert die Aktivierung von privilegierten Containern für den Plan/die Kachel und die Änderung des yaml hostPath wie folgt für Allinone, Controller, Enforcer:

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====

*Master-Knoten Taints und Toleranzen*

Alle Taint-Informationen müssen übereinstimmen, um Enforcer auf Knoten zu planen. So prüfen Sie die Taint-Informationen eines Knotens (z. B. Master):

[,shell]
----
kubectl get node taintnodename -o yaml
----

Beispielhafte Ausgabe:

[,yaml]
----
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue
----

Wenn es zusätzliche Färbungen wie oben gibt, fügen Sie diese in den Abschnitt "Beispiel-Yaml-Toleranzen" ein:

[,yaml]
----
spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue
----

== Verwendung von Knotenetiketten für Manager- und Controller-Knoten

Um zu kontrollieren, auf welchen Knoten der Manager und der Controller eingesetzt werden, kennzeichnen Sie jeden Knoten. Ersetzen Sie nodename durch den entsprechenden Knotennamen ('`kubectl get nodes`'). Anmerkung: Standardmäßig plant Kubernetes keine Pods auf dem Master-Knoten.

[,shell]
----
kubectl label nodes nodename nvcontroller=true
----

Fügen Sie dann einen nodeSelector in die yaml-Datei für die Bereitstellungsabschnitte Manager und Controller ein. Zum Beispiel:

[,yaml]
----
          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always
----

Um zu verhindern, dass der Enforcer auf einem Controller-Knoten eingesetzt wird, wenn es sich um einen dedizierten Management-Knoten handelt (ohne zu überwachende Anwendungscontainer), fügen Sie dem Abschnitt Enforcer yaml eine nodeAffinity hinzu. Zum Beispiel:

[,yaml]
----
  app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:
----

== Laufende Aktualisierungen

Orchestrierungswerkzeuge wie Kubernetes, RedHat OpenShift und Rancher unterstützen rollierende Updates mit konfigurierbaren Richtlinien. Sie können diese Funktion nutzen, um die {product-name} Container zu aktualisieren. Vor allem muss sichergestellt werden, dass mindestens ein Controller (oder Allinone) in Betrieb ist, damit Richtlinien, Protokolle und Verbindungsdaten nicht verloren gehen. Stellen Sie sicher, dass zwischen den Aktualisierungen der Container mindestens 120 Sekunden liegen, damit ein neuer Leader gewählt und die Daten zwischen den Controllern synchronisiert werden können.

In den bereitgestellten Beispielen für die Bereitstellung ist die Rolling-Update-Richtlinie bereits konfiguriert. Wenn Sie die Aktualisierung über das {product-name} Helm-Diagramm vornehmen, ziehen Sie bitte das neueste Diagramm heran, um neue Funktionen wie die Zugangskontrolle ordnungsgemäß zu konfigurieren, und löschen Sie die alte Clusterrolle und die Clusterrollenbindung für {product-name}. Wenn Sie über Kubernetes aktualisieren, können Sie mit den folgenden Beispielbefehlen manuell auf eine neue Version aktualisieren.

=== Beispiel für ein rollendes Kubernetes-Update

Für Upgrades, bei denen lediglich eine neue Image-Version aktualisiert werden muss, können Sie diesen einfachen Ansatz verwenden.

Wenn Ihr Deployment- oder Daemonset bereits läuft, können Sie die yaml-Datei auf die neue Version ändern und dann das Update anwenden:

[,shell]
----
kubectl apply -f <yaml file>
----

So aktualisieren Sie über die Befehlszeile auf eine neue Version von {product-name}.

Für Controller als Deployment (gilt auch für Manager)

[,shell]
----
kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:<version> -n neuvector
----

Für jeden Container als DaemonSet:

[,shell]
----
kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:<version>
----

So überprüfen Sie den Status der laufenden Aktualisierung:

[,shell]
----
kubectl rollout status -n neuvector ds/neuvector-enforcer-pod
kubectl rollout status -n neuvector deployment/neuvector-controller-pod
----

So machen Sie die Aktualisierung rückgängig:

[,shell]
----
kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod
kubectl rollout undo -n neuvector deployment/neuvector-controller-pod
----

== REST-API in Kubernetes verfügbar machen

Um die REST-API für den Zugriff von außerhalb des Kubernetes-Clusters freizugeben, finden Sie hier eine Beispiel-Yaml-Datei:

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-rest
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Weitere Informationen über die REST-API finden Sie im Abschnitt Automatisierung.

== Kubernetes-Bereitstellung im nicht-privilegierten Modus

Die folgenden Anweisungen können verwendet werden, um {product-name} ohne Verwendung von Containern im privilegierten Modus einzusetzen. Der Controller befindet sich bereits im nichtprivilegierten Modus und die Enforcer-Bereitstellung sollte geändert werden, wie in den folgenden Auszügen gezeigt wird.

Vollstrecker:

[,yaml]
----
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
        # this line is required to be added if k8s version is pre-v1.19
        # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
----

== Kubernetes-Bereitstellung YAML für v5.4.2 und höher

Das folgende YAML-Beispiel ist für Versionen ab 5.4.2, bei denen wir die internen Zertifikate auf Controller-, Enforcer- und Scanner-Pods einbinden müssen, da wir keine hardcodierten Zertifikate mehr unterstützen. Erstellen Sie das interne Zertifikatsgeheimnis über den angegebenen Link, bevor Sie es bereitstellen: xref:internal.adoc[Ersetzen interner Zertifikate].

.Klicken Sie hier für Details
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            privileged: true
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
          volumeMounts:
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      restartPolicy: Always
      volumes:
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert
---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

Das folgende Beispiel ist eine vollständige Bereitstellungsreferenz (Kubernetes 1.19+).

.Klicken Sie hier für Details
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

== PKS Veränderung

[NOTE]
====
PKS wird in der Praxis getestet und erfordert die Aktivierung von privilegierten Containern für den Plan/die Kachel und die Änderung des yaml hostPath wie folgt für Allinone, Enforcer:

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====
