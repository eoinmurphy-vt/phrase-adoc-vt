= Exigences système
:revdate: 2025-06-02
:page-revdate: {revdate}
:page-opendocs-origin: /01.basics/02.requirements/02.requirements.md
:page-opendocs-slug: /bases/exigences

== Exigences système

[cols="1,1,1,1,4", options="header"]
|===
| Composant | # d'instances | vCPU recommandé | Mémoire minimale | Notes

| Contrôleur
| min. 1 +
3 pour HA (nombre impair uniquement)
| 1
| 1 GO
| Le cœur vCPU peut être partagé

| Enforcer
| 1 par nœud/VM
| 1+
| 1 GO
| Un ou plusieurs vCPU dédiés pour un débit réseau plus élevé en mode Protect

| Scanner
| min. 1 +
2+ pour HA/Performance
| 1
| 1 GO
| Le cœur CPU peut être partagé pour des charges de travail standard. +
Dédier 1 ou plusieurs CPU pour un scan d'images à fort volume (10k+). +
La numérisation des images du registre est effectuée par le scanner et gérée par le contrôleur, et l'image est récupérée par le scanner et étendue en mémoire. +
La recommandation minimale de mémoire suppose que les images à numériser ne dépassent pas 0,5 Go. +
Lors de la numérisation d'images de plus de 1 Go, la mémoire du scanner doit être calculée en prenant la plus grande taille d'image et en ajoutant 0,5 Go. +
Exemple - taille d'image la plus grande = 1,3 Go, la mémoire du conteneur du scanner doit être de 1,8 Go.

| Gestionnaire
| min 1 +
2+ pour HA
| 1
| 1 GO
| vCPU peut être partagé
|===

* Pour la sauvegarde de configuration/HA, un PVC RWX de 1 Gi ou plus. Voir xref:production.adoc#_backups_and_persistent_data[Section des sauvegardes et des données persistantes] pour plus de détails.
* Navigateur recommandé : Chrome pour de meilleures performances

== Plateformes prises en charge

* Distributions linux officiellement prises en charge, SUSE Linux, Ubuntu, CentOS/Red Hat (RHEL), Debian, CoreOS, AWS Bottlerocket et Photon.
* Architectures AMD64 et Arm
* CoreOS est pris en charge (novembre 2023) pour la numérisation CVE via le tableau de correspondance RHEL fourni par RedHat. Une fois qu'un flux officiel est publié par RedHat pour CoreOS, il sera pris en charge.
* Systèmes de gestion de conteneurs conformes à Kubernetes et Docker officiellement pris en charge. Les plateformes suivantes sont testées avec chaque version de {product-name} : Kubernetes 1.19-1.32, SUSE Rancher (RKE, RKE2, K3s, etc.), RedHat OpenShift 4.6-4.16 (3.x à 4.12 pris en charge avant {product-name} 5.2.x), Google GKE, Amazon EKS, Microsoft Azure AKS, IBM IKS, docker natif, docker swarm. Les plateformes conformes à Kubernetes et Docker suivantes sont prises en charge et ont été vérifiées pour fonctionner avec {product-name} : VMware Photon et Tanzu, SUSE CaaS, Oracle OKE, Mirantis Kubernetes Engine, Nutanix Kubernetes Engine, docker UCP/DataCenter, docker Cloud.
* Version d'exécution de Docker : 1.9.0 et supérieur ; version de l'API Docker : 1.21, CE et EE.
* Exécutions Containerd et CRI-O (nécessite des modifications des chemins de volume dans les fichiers yamls d'exemple). Voir les modifications requises pour Containerd dans la section de déploiement Kubernetes et CRI-O dans la section de déploiement OpenShift.
* {product-name} est compatible avec la plupart des CNI commercialement pris en charge. Les tests officiels et le support concernent openshift ovs (sous-réseau/multitenant), calico, flannel, cilium, antrea et les clouds publics (gke, aks, iks, eks). Le support pour Multus a été ajouté dans v5.4.0.
* Console : Navigateur Chrome ou Firefox recommandé. IE 11 non pris en charge en raison de problèmes de performance.
* Minikube est pris en charge pour une évaluation initiale simple mais pas pour une preuve de concept complète. Voir ci-dessous les modifications requises pour que le yaml Allinone fonctionne sur Minikube.

Note sur AWS Bottlerocket : Doit changer le chemin du socket containerd spécifique à Bottleneck. Veuillez consulter la section de déploiement Kubernetes pour plus de détails.

== Non pris en charge

* GKE Autopilot.
* AWS ECS n'est plus supporté. (REMARQUE : Aucune fonctionnalité n'a été activement supprimée pour faire fonctionner {product-name} sur les déploiements ECS. Cependant, les tests sur ECS ne sont plus effectués par SUSE. Bien que la protection des charges de travail ECS avec {product-name} fonctionnera probablement comme prévu, les problèmes ne seront pas examinés.)
* Docker sur Mac
* Docker sur Windows
* Rkt (container linux) de CoreOS
* AppArmor sur les environnements K3S / SLES. Certaines configurations peuvent entrer en conflit avec {product-name} et provoquer des erreurs de scanner ; AppArmor doit être désactivé lors du déploiement de {product-name}.
* IPv6 n'est pas supporté
* VMWare Integrated Containers (VIC) sauf en mode imbriqué
* CloudFoundry
* Console : IE 11 non pris en charge en raison de problèmes de performance.
* Hôte de conteneur imbriqué dans des outils de conteneur utilisés pour des tests simples. Par exemple, déploiement d'un cluster Kubernetes utilisant 'kind' https://kind.sigs.k8s.io/docs/user/configuration/.

[NOTE]
====
PKS est testé sur le terrain et nécessite l'activation des conteneurs privilégiés pour le plan/tuile, et le changement du yaml hostPath comme suit pour Allinone, Controller, Enforcer :

[,yaml]
----
            hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====

[NOTE]
====
{product-name} prend en charge l'exécution sur des VM basées sur Linux sur Mac/Windows utilisant Vagrant, VirtualBox, VMware ou d'autres environnements virtualisés.
====


== Minikube

Veuillez apporter les modifications suivantes au yaml de déploiement Allinone.

[,yaml]
----
apiVersion: apps/v1 <<-- required for k8s 1.19
kind: DaemonSet
metadata:
 name: neuvector-allinone-pod
 namespace: neuvector
spec:
 selector: <-- Added
 matchLabels: <-- Added
 app: neuvector-allinone-pod <-- Added
 minReadySeconds: 60
...
 nodeSelector: <-- DELETE THIS LINE
 nvallinone: "true" <-- DELETE THIS LINE
apiVersion: apps/v1 <<-- required for k8s 1.19
kind: DaemonSet
metadata:
 name: neuvector-enforcer-pod
 namespace: neuvector
spec:
 selector: <-- Added
 matchLabels: <-- Added
 app: neuvector-enforcer-pod <-- Added
----

== Performance et mise à l'échelle

Comme toujours, la planification des performances pour les conteneurs {product-name} dépendra de plusieurs facteurs, y compris :

* (Controller & Scanner) Nombre et taille des images dans le registre à scanner (par le Scanner) initialement
* (Enforcer) Mode de services (Découvrir, Surveiller, Protéger), où le mode Protéger fonctionne comme un pare-feu en ligne
* (Enforcer) Type de connexions réseau pour les charges de travail en mode Protéger

En mode Surveiller (filtrage réseau similaire à un miroir/tap), il n'y a pas d'impact sur les performances et l'Enforcer gère le trafic à la vitesse de ligne, générant des alertes si nécessaire. En mode Protéger (pare-feu en ligne), l'Enforcer nécessite du CPU et de la mémoire pour filtrer les connexions avec une inspection approfondie des paquets et les maintenir pour déterminer si elles doivent être bloquées/abandonnées. En général, avec 1 Go de mémoire et un CPU partagé, l'Enforcer devrait être capable de gérer la plupart des environnements en mode Protéger.

Pour les environnements sensibles au débit ou à la latence, de la mémoire supplémentaire et/ou un cœur CPU dédié peuvent être alloués au conteneur {product-name} Enforcer.

Pour l'optimisation des performances du Contrôleur et du Scanner pour le scan de registre, voir les Exigences Système ci-dessus.

Pour des conseils supplémentaires sur les performances et le dimensionnement, voir la section xref:production.adoc#_best_practices_tips_qa_for_deploying_and_managing_suse_security[Intégration/Meilleures Pratiques].

=== Débit

Comme le montre le graphique ci-dessous, des tests de référence de débit de base ont montré un débit maximum de 1,3 Gbps PAR NŒUD sur une petite instance de cloud public avec 4 cœurs CPU. Par exemple, un cluster de 10 nœuds pourrait alors gérer un maximum de 13 Gbps de débit pour l'ensemble du cluster pour les services en mode Protéger.

image:throughput.png[Débit]

Ce débit serait projeté pour augmenter à mesure qu'un CPU dédié est attribué à l'Enforcer, ou que la vitesse du CPU change, et/ou que de la mémoire supplémentaire est allouée. Encore une fois, l'évolutivité dépendra du type de trafic réseau/application des charges de travail.

=== Latence

La latence est une autre métrique de performance qui dépend du type de connexions réseau. Tout comme le débit, la latence n'est pas affectée en mode Surveiller, seulement pour les services en mode Protéger (pare-feu en ligne). De petits paquets ou des services simples/rapides généreront une latence plus élevée de {product-name} en pourcentage, tandis que des paquets plus grands ou des services nécessitant un traitement complexe montreront un pourcentage de latence ajoutée plus faible par l'{product-name} enforcer.

Le tableau ci-dessous montre la latence moyenne de 2 à 10 % mesurée à l'aide de l'outil de référence Redis. Le Benchmark Redis utilise des paquets assez petits, donc la latence avec des paquets plus grands serait attendue pour être plus faible.

|===
| Test | Moniteur | Protéger | Latence

| PING_INLINE
| 34 904
| 31 603
| 9,46 %

| RÉGLER
| 38 618
| 36 157
| 6,37 %

| OBTENIR
| 36 055
| 35 184
| 2,42 %

| LPUSH
| 39 853
| 35 994
| 9,68 %

| RPUSH
| 37 685
| 36 010
| 4,45%

| LPUSH (Benchmark LRANGE)
| 37 399
| 35 220
| 5,83%

| LRANGE_100
| 25 539
| 23 906
| 6,39%

| LRANGE_300
| 13 082
| 12 277
| 6,15%
|===

Le benchmark ci-dessus montre le TPS moyen du mode Protect par rapport au mode Monitor, et la latence ajoutée pour le mode Protect pour plusieurs tests dans le benchmark. Le principal moyen de réduire la latence réelle (microsecondes) en mode Protect est d'exécuter sur un système avec un CPU plus rapide. Vous pouvez trouver plus de détails sur cet outil de benchmark Redis open source à https://redis.io/topics/benchmarks.

=== Ajout de contraintes de mise à l'échelle pour des environnements de charge de travail importants

Lors de l'installation de NeuVector, si votre système d'exploitation hôte a une grande quantité de charges de travail, les pods NeuVector Enforcer peuvent échouer à se lancer en essayant d'ouvrir le grand volume de fichiers en raison de la surveillance de l'hôte des pods. Cela peut également provoquer des échecs du serveur RKE2 en raison des grandes quantités de fichiers ouverts.

Comme solution de contournement pour les environnements de charge de travail importants, vous devez créer un fichier tel que `example-fs-max.conf` à l'emplacement `/etc/sysctl.d/` et ajouter des contraintes de mise à l'échelle avec la configuration suivante :

[,shell]
----
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=524288
fs.filemax=5000
----

Ensuite, assurez-vous que la configuration est appliquée avec un redémarrage via la commande suivante :

[,shell]
----
systemctl restart systemd-sysctl
----