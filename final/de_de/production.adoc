= Test_TwoDeploying \{product-name}
:revdate: 2024-09-27
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/01.production/01.production.md
:page-opendocs-slug: /deploying/production

== Test_TwoPlanning Deployments

Die \{product-name} Container in einem Standard-Deployment umfassen den Controller, Manager, Durchsetzer, Scanner und Aktualisierer. Die Platzierung, wo diese Container (auf welchen Knoten) bereitgestellt werden, muss berücksichtigt werden, und geeignete Labels, Taints oder Toleranzen müssen erstellt werden, um sie zu steuern.

Der Durchsetzer sollte auf jedem Host/Knoten bereitgestellt werden, auf dem Anwendungscontainer, die von \{product-name} überwacht und geschützt werden sollen, ausgeführt werden.

Der Controller verwaltet den Cluster von Durchsetzern und kann auf demselben Knoten wie ein Durchsetzer oder auf einem separaten Verwaltungs-Knoten bereitgestellt werden. Der Manager sollte auf dem Knoten bereitgestellt werden, auf dem der Controller läuft, und wird Konsolenzugriff auf den Controller bereitstellen. Andere erforderliche \{product-name} Container wie der Manager, Scanner und Aktualisierer werden im Best Practices-Leitfaden, auf den unten verwiesen wird, detaillierter beschrieben.

Wenn Sie dies noch nicht getan haben, ziehen Sie die Bilder vom \{product-name} Docker Hub.

Die Bilder befinden sich im \{product-name} Docker Hub-Registry. Verwenden Sie das entsprechende Versions-Tag für den Manager, Controller, Durchsetzer und lassen Sie die Version für Scanner und Aktualisierer als 'latest'. Zum Beispiel:

* neuvector/manager:5.3.2
* neuvector/controller:5.3.2
* neuvector/enforcer:5.3.2
* neuvector/scanner:latest
* neuvector/updater:latest

Bitte stellen Sie sicher, dass Sie die Bildreferenzen in den entsprechenden yaml-Dateien aktualisieren.

Wenn Sie mit dem aktuellen \{product-name} Helm-Chart (v1.8.9+) bereitstellen, sollten die folgenden Änderungen an values.yml vorgenommen werden:

* Aktualisieren Sie das Registry auf docker.io
* Aktualisieren Sie die Bildnamen/-tags auf die aktuelle Version auf Docker Hub, wie oben gezeigt
* Lassen Sie die imagePullSecrets leer

=== Test_TwoBeste Praktiken, Tipps, Q&A für Bereitstellung und Verwaltung von \{product-name}

Laden Sie dieses xref:attachment$NV_Onboarding_5.0.pdf[Dokument zu den besten Praktiken für die Bereitstellung] herunter und überprüfen Sie es auf Tipps zu Leistung und Dimensionierung, besten Praktiken und häufig gestellten Fragen zu Bereitstellungen.

== Test_TwoBereitstellung mit Helm oder Operatoren

Automatisierte Bereitstellung mit Helm finden Sie unter https://github.com/neuvector/neuvector-helm.

Die Bereitstellung mit einem Operator, einschließlich des RedHat-zertifizierten Operators und des Kubernetes-Community-Operators, wird unterstützt, mit einer allgemeinen Beschreibung xref:operators.adoc[hier]. Der \{product-name} RedHat-Operator befindet sich unter https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, und der Community-Operator unter https://operatorhub.io/operator/neuvector-operator.

== Test_TwoBereitstellung mit ConfigMap

Die automatisierte Bereitstellung auf Kubernetes wird mit einer ConfigMap unterstützt. Bitte sehen Sie sich den Abschnitt xref:configmap.adoc[Bereitstellung mit ConfigMap] für weitere Details an.

== Test_TwoBereitstellung der Controller

Wir empfehlen, mehrere Controller für eine Hochverfügbarkeitskonfiguration (HA) zu betreiben. Die Controller verwenden das konsensbasierte RAFT-Protokoll, um einen Anführer zu wählen, und wenn der Anführer ausfällt, um einen anderen Anführer zu wählen. Aus diesem Grund sollte die Anzahl der aktiven Controller eine ungerade Zahl sein, zum Beispiel 3, 5, 7 usw.

== Test_TwoController HA

Die Controller synchronisieren alle Daten untereinander, einschließlich Konfiguration, Richtlinien, Gespräche, Ereignisse und Benachrichtigungen.

Wenn der primäre aktive Controller ausfällt, wird automatisch ein neuer Anführer gewählt und übernimmt.

Treffen Sie besondere Vorkehrungen, um sicherzustellen, dass immer ein Controller läuft und bereit ist, insbesondere während Updates und Neustarts des Host-Betriebssystems oder der Orchestrierungsplattform.

== Test_ZweiBackups und persistente Daten

Stellen Sie sicher, dass Sie die Konfigurationsdatei regelmäßig aus der Konsole exportieren und als Backup speichern.

Wenn Sie mehrere Controller in einer HA-Konfiguration betreiben, werden alle Daten zwischen den Controllern synchronisiert, solange mindestens ein Controller immer aktiv ist.

Wenn Sie Protokolle wie Verstöße, Bedrohungen, Schwachstellen und Ereignisse speichern möchten, aktivieren Sie bitte den SYSLOG-Server in den Einstellungen.

\{product-name} unterstützt persistente Daten für die \{product-name}-Richtlinie und -Konfiguration. Dies konfiguriert ein Echtzeit-Backup, um ein Volume unter /var/neuvector/ vom Controller-Pod zu mounten. Der Hauptanwendungsfall ist, wenn das persistente Volume gemountet ist, dass die Konfiguration und Richtlinie zur Laufzeit im persistenten Volume gespeichert werden. Im Falle eines totalen Ausfalls des Clusters wird die Konfiguration automatisch wiederhergestellt, wenn der neue Cluster erstellt wird. Konfiguration und Richtlinie können auch manuell vom /var/neuvector/ Volume wiederhergestellt oder entfernt werden.

[IMPORTANT]
====
Wenn ein persistentes Volume nicht gemountet ist, speichert \{product-name} die Konfiguration oder Richtlinie NICHT als persistente Daten. Stellen Sie sicher, dass Sie die Konfiguration und Richtlinie des Controllers sichern, bevor Sie den All-in-One- oder Controller-Container stoppen. Dies kann in `Settings -> Configuration` erfolgen. Alternativ kann der Controller in einer HA-Konfiguration mit 3 oder 5 laufenden Controllern bereitgestellt werden, wobei die Richtlinie mit anderen Controllern bestehen bleibt, während einer aktualisiert wird.
====

=== Test_ZweiPersistentes Volume Beispiel

Das im Cluster definierte PersistentVolume ist für die Unterstützung von persistenten Volumes erforderlich. Die Anforderung für \{product-name} ist, dass die accessModes ReadWriteMany (RWX) sein müssen. Nicht alle Speichertypen unterstützen den RWX-Zugriffsmodus. Zum Beispiel müssen Sie auf GKE möglicherweise ein RWX-persistentes Volume mit NFS-Speicher erstellen.

Sobald das PersistentVolume erstellt ist, muss ein PersistentVolumeClaim wie unten für den Controller erstellt werden. Derzeit wird das persistente Volume nur für die \{product-name}-Konfigurations-Backup-Dateien im Controller (Richtlinien, Regeln, Benutzerdaten, Integrationen usw.) und die Ergebnisse des Registry-Scans verwendet.

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
----

Hier ist ein Beispiel für IBM Cloud:

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neuvector-data
  namespace: neuvector
  labels:
    billingType: "hourly"
    region: us-south
    zone: sjc03
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
      iops: "100"
  storageClassName: ibmc-file-retain-custom
----

Nachdem der Persistent Volume Claim erstellt wurde, ändern Sie die \{product-name} Beispiel-YAML-Datei wie unten gezeigt (alter Abschnitt auskommentiert):

[,yaml]
----
...
spec:
  template:
    spec:
      volumes:
        - name: nv-share
#         hostPath:                        // replaced by persistentVolumeClaim
#           path: /var/neuvector        // replaced by persistentVolumeClaim
          persistentVolumeClaim:
            claimName: neuvector-data
----

Fügen Sie auch die folgende Umgebungsvariable in den Controller- oder All-in-one-Beispiel-YAMLs für die Unterstützung von persistentem Speicher hinzu. Dies wird den Controller dazu bringen, die Backup-Konfiguration beim Start zu lesen.

[,yaml]
----
            - name: CTRL_PERSIST_CONFIG
----

=== Test_TwoConfigMaps und Persistenter Speicher

Sowohl die ConfigMaps als auch das Backup des persistenten Speichers werden nur gelesen, wenn ein neuer \{product-name} Cluster bereitgestellt wird oder der Cluster ausfällt und neu gestartet wird. Sie werden während der Rolling-Upgrades nicht verwendet.

Das Backup der Konfiguration des persistenten Speichers wird zuerst gelesen, dann werden die ConfigMaps angewendet, sodass die Einstellungen der ConfigMap Vorrang haben. Alle Einstellungen der ConfigMap (z.B. Updates) werden ebenfalls im persistenten Speicher gespeichert.

Für weitere Informationen siehe den Abschnitt xref:configmap.adoc[ConfigMaps].

== Test_TwoAktualisierung der CVE-Sicherheitsdatenbank in der Produktion

Bitte sehen Sie sich jeden Beispielabschnitt für Anweisungen an, wie die CVE-Datenbank aktualisiert werden kann.

Die Version der CVE-Datenbank kann in der Konsole im Tab "Schwachstellen" angezeigt werden. Sie können auch das Updater-Container-Image inspizieren.

[,shell]
----
docker inspect neuvector/updater
----

[,json]
----
"Labels": {
                "neuvector.image": "neuvector/updater",
                "neuvector.role": "updater",
                "neuvector.vuln_db": "1.255"
            }
----

Nach dem Ausführen des Updates, inspizieren Sie die Protokolle des Controllers/All-in-one nach 'version.' Zum Beispiel in Kubernetes:

[,shell]
----
kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version

...
2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576
2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576
----

== Test_TwoZugriff auf die Konsole

Standardmäßig wird die Konsole als Dienst auf Port 8443 oder als NodePort mit einem zufälligen Port auf jedem Host bereitgestellt. Bitte sehen Sie sich den ersten Abschnitt Grundlagen -> xref:rest-api.adoc[Verbinden mit dem Manager] für Optionen an, um HTTPS zu deaktivieren oder auf die Konsole über eine Unternehmensfirewall zuzugreifen, die den Port 8443 für den Konsolenzugriff nicht zulässt.

== Test_TwoVerwaltung von Host-Updates oder Auto-Scaling-Knoten mit einem Pod-Störungshaushalt

Wartungs- oder Skalierungsaktivitäten können die Controller auf Knoten beeinträchtigen. Öffentliche Cloud-Anbieter unterstützen die Möglichkeit, Knoten automatisch zu skalieren, was Pods einschließlich der \{product-name} Controller dynamisch entfernen kann. Um Unterbrechungen der Controller zu verhindern, kann ein \{product-name} Pod-Störungshaushalt erstellt werden.

Erstellen Sie beispielsweise die folgende Datei nv_pdb.yaml, um sicherzustellen, dass jederzeit mindestens 2 Controller ausgeführt werden.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

Dann

[,shell]
----
kubectl create -f nv_pdb.yaml
----

Für weitere Details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/

== Test_TwoDeploy ohne privilegierten Modus

Auf einigen Systemen wird die Bereitstellung ohne Verwendung des privilegierten Modus unterstützt. Diese Systeme müssen seccom-Funktionen und das Setzen des AppArmor-Profils unterstützen.

Siehe den Abschnitt über xref:docker.adoc[Docker-Bereitstellung] für Beispiel-Compose-Dateien.

== Test_TwoMulti-Standorte, Multi-Cluster-Architektur

Für Unternehmen mit mehreren Standorten, an denen ein separater \{product-name} Cluster für jeden Standort bereitgestellt werden kann, wird die folgende Referenzarchitektur vorgeschlagen. Jeder Cluster hat sein eigenes Set von Controllern und wird separat verwaltet.

image:multisite.png[Multi-Standorte]

Siehe eine detailliertere Beschreibung in dieser Datei >
xref:attachment$multisite.pdf[\{product-name} Multi-Site-Architektur]
