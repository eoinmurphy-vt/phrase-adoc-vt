= Bereitstellung von SUSE-Sicherheit in der Public Cloud
:revdate: 2025-06-25
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/08.publick8s/08.publick8s.md
:page-opendocs-slug: /deploying/publick8s

== {product-name} auf einem Public Cloud Kubernetes-Dienst bereitstellen

{product-name} auf einem beliebigen öffentlichen Cloud K8s-Dienst wie AWS EKS, Azure AKS, IBM Cloud K8s, Google Cloud, Alibaba Cloud oder Oracle Cloud bereitstellen. {product-name} hat das Amazon EKS Anywhere Konformitäts- und Validierungsframework bestanden und ist daher eine validierte Lösung, die als Add-on für EKS-Anywhere auf Snowball Edge-Geräten über die AWS-Konsole verfügbar ist.

Zuerst erstellen Sie Ihren K8s-Cluster und bestätigen den Zugriff mit `+kubectl get nodes+`.

Um {product-name} bereitzustellen, verwenden Sie die Beispielbereitstellungsanweisungen und -beispiele aus dem Kubernetes-Bereich der Produktionsbereitstellung. Bearbeiten Sie die Beispiel-yaml, wenn Sie {product-name} Bilder aus einem lokalen oder Cloud-Registry wie ECR oder ACR abrufen.

Einige Cloud-Anbieter haben integrierte Lastenausgleicher, die einfach bereitgestellt werden können, indem Sie `+Type: LoadBalancer+` anstelle von NodePort für die {product-name} webui verwenden.

{product-name} unterstützt auch die Helm-basierte Bereitstellung mit einem Helm-Chart unter https://github.com/neuvector/neuvector-helm.

=== Netzwerkzugang

Stellen Sie sicher, dass der interne und externe Ingress-Zugriff ordnungsgemäß konfiguriert ist. Für den NodePort-Dienst muss der zufällige Port im Bereich 3xxxx von außen auf einer öffentlichen IP eines Arbeits- oder Masterknotens zugänglich sein. Sie können die Konsole über die öffentliche IP-Adresse eines beliebigen Arbeitsknotens und diesen Port (NodePort) oder die öffentliche IP des Lastenausgleichers und den Standardport 8443 aufrufen. Sie können die IP/Port mit folgendem Befehl anzeigen:

[,shell]
----
kubectl get svc -n neuvector
----

Die meisten K8s-Dienste ermöglichen automatisch alle inter-pod / inter-cluster Kommunikation zwischen Knoten, was auch den {product-name} Containern (Durchsetzer, Controller, Manager) ermöglicht, innerhalb des Clusters zu kommunizieren.

Die Beispiel-Kubernetes-yaml-Datei wird einen Manager und 3 Controller bereitstellen. Es wird einen Durchsetzer auf jedem Knoten als Daemonset bereitstellen. Hinweis: Es wird nicht empfohlen, (skalieren) mehr als einen Manager hinter einem Lastenausgleicher bereitzustellen, da dies potenzielle Probleme mit dem Sitzungsstatus verursachen kann.

== Microsoft Azure AKS

Beim Bereitstellen eines K8s-Clusters auf Azure ist die Standardeinstellung für Kubernetes RBACs deaktiviert. Bitte aktivieren Sie RBACs, um die Cluster-Admin-Clusterrolle zu aktivieren, andernfalls müssen Sie diese später manuell erstellen, um Helm-basierte Bereitstellungen zu unterstützen.

== Google Cloud Platform / GKE

Sie können die integrierten Lastenausgleicher verwenden, die einfach bereitzustellen sind, indem Sie &apos;`+Type: LoadBalancer+`&apos; anstelle von NodePort für die {product-name} webui verwenden. Die Konfiguration von persistentem Speicher mit dem Typ RWM (read write many) kann erfordern, dass Sie einen Speicherdienst wie NFS erstellen, bevor Sie {product-name} bereitstellen.

{product-name} erfordert ein SDN-Plugin wie flannel, weave oder calico.

Verwenden Sie die Umgebungsvariable NV_PLATFORM_INFO mit dem Wert platform=Kubernetes:GKE, um {product-name} zu aktivieren, um GKE-spezifische Aktionen wie das Ausführen der GKE Kubernetes CIS Benchmarks durchzuführen.

== GKE Auto Pilot Unterstützung

Die GKE Auto Pilot Unterstützung ist verfügbar mit NeuVector v5.4.3 und später. Bitte folgen Sie den folgenden Schritten, um NeuVector im Auto Pilot-Cluster bereitzustellen.

Ein `+AllowlistSynchronizer+` sollte im Cluster erstellt werden, bevor Sie NeuVector bereitstellen. Hier ist die Konfigurations-YAML mit `+allowlistPath+` und dem Befehl zum Anwenden der YAML:

Beispielbefehl zum Anwenden von YAML:

[,shell]
----
kubectl apply -f allowlist.yaml
----

Beispiel-YAML-Konfiguration:

[,yaml]
----
apiVersion: auto.gke.io/v1
kind: AllowlistSynchronizer
metadata:
  name: neuvector-allowlist
spec:
  allowlistPaths:
  - SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
  - SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
----

Überprüfen Sie nach Ausführen des Befehls `+kubectl apply -f <YAML file>+`, ob das `+AllowlistSynchronizer+` bereit ist.

Beispielbefehl:

[,shell]
----
kubectl get AllowlistSynchronizer neuvector-allowlist -o yaml
----

Beispiel-YAML-Konfiguration:

[,yaml]
----
apiVersion: auto.gke.io/v1
kind: AllowlistSynchronizer
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"auto.gke.io/v1","kind":"AllowlistSynchronizer","metadata":{"annotations":{},"name":"neuvector-allowlist"},"spec":{"allowlistPaths":["SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml","SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml"]}}
  creationTimestamp: "2025-04-28T18:17:16Z"
  generation: 1
  name: neuvector-allowlist
  resourceVersion: "13326"
  uid: 3e425c28-9bef-4459-b769-381d974f17f6
spec:
  allowlistPaths:
  - SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
  - SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
status:
  conditions:
  - lastTransitionTime: "2025-04-28T18:17:17Z"
    message: Synchronization completed successfully; allowlists up to date
    observedGeneration: 1
    reason: SyncSuccessful
    status: "True"
    type: Ready
  lastSyncAttempt: "2025-04-28T18:17:17Z"
  managedAllowlistStatus:
  - filePath: SUSE/neuvector-enforcer/v1.0.0/suse-neuvector-enforcer.yaml
    generation: 1
    lastSuccessfulSync: "2025-04-28T18:17:16Z"
    phase: Installed
  - filePath: SUSE/neuvector-scanner/v1.0.0/suse-neuvector-scanner.yaml
    generation: 1
    lastSuccessfulSync: "2025-04-28T18:17:17Z"
    phase: Installed
----

Die `+override.yaml+` Datei unten muss verwendet werden, um NeuVector im GKE Autopilot-Cluster bereitzustellen, wenn Helm verwendet wird.

[,yaml]
----
cve:
  scanner:
    podLabels:
      # The scanner allowlist should be mapped with scanner deployment workload.
      cloud.google.com/matching-allowlist: suse-neuvector-scanner
    resources:
      # Below are the tested limits for scanner deployment in GKE Auto-Pilot cluster for scanner pod.
      limits:
        ephemeral-storage: "3Gi"
      requests:
        ephemeral-storage: "2Gi"
enforcer:
  podLabels:
     # The enforcer allowlist should be mapped with the enforcer daemon set workload.
    cloud.google.com/matching-allowlist: suse-neuvector-enforcer
----

Wenn Sie die YAML-Bereitstellung verwenden, fügen Sie bitte die `+podLabels+` und Ressourcengrenzen in den `+enforcer+` und `+scanner+` YAML-Konfigurationen entsprechend hinzu.

Um mehr über das `+allowlistSynchronizer+` zu erfahren, sehen Sie sich bitte die https://cloud.google.com/kubernetes-engine/docs/how-to/run-autopilot-partner-workloads[GKE-Dokumentation] an.

== Verwaltung von Auto-Scaling-Knoten mit einem Pod-Störungshaushalt

Öffentliche Cloud-Anbieter unterstützen die Möglichkeit, Knoten automatisch zu skalieren, was das dynamische Entfernen von Pods, einschließlich der {product-name} Controller, ermöglichen kann. Um Störungen der Controller zu verhindern, kann ein {product-name} Pod-Störungshaushalt erstellt werden.

Erstellen Sie beispielsweise die untenstehende Datei nv_pdr.yaml, um sicherzustellen, dass jederzeit mindestens 2 Controller ausgeführt werden.

[,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: neuvector-controller-pdb
  namespace: neuvector
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: neuvector-controller-pod
----

Dann

[,shell]
----
kubectl create -f nv_pdr.yaml
----

Für weitere Details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
