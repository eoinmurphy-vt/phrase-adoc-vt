= RedHat OpenShift
:revdate: 2025-06-05
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/04.openshift/04.openshift.md
:page-opendocs-slug: /déployer/openshift

== Déployer des composants {product-name} séparés avec RedHat OpenShift

{product-name} est compatible avec les plug-ins SDN ovs standard ainsi qu'avec d'autres tels que flannel, weave ou calico. Les exemples ci-dessous supposent qu'un plug-in ovs standard est utilisé. Cela suppose également qu'un registre docker local sera utilisé (voir les instructions à la fin pour créer le secret pour tirer dynamiquement depuis neuvector ou Docker Hub).

{product-name} prend en charge le déploiement basé sur Helm avec un https://github.com/neuvector/neuvector-helm[chart Helm] à https://github.com/neuvector/neuvector-helm. L'Opérateur {product-name} peut également être utilisé pour déployer et est basé sur le chart Helm. Pour déployer les dernières versions de conteneurs {product-name} en utilisant un Opérateur, veuillez utiliser soit l'Opérateur certifié Red Hat depuis Operator Hub, soit l'opérateur communautaire, comme détaillé dans la xref:operators.adoc[section Opérateur].

Pour déployer manuellement, commencez par tirer les conteneurs {product-name} appropriés depuis le registre {product-name} dans votre registre local. Remarque : l'image du scanner doit être tirée régulièrement pour les mises à jour de la base de données CVE depuis {product-name}.

=== Images {product-name} sur Docker Hub

Les images sont sur le registre Docker Hub {product-name}. Utilisez le tag de version approprié pour le gestionnaire, le contrôleur, l'exécuteur, et laissez la version comme 'latest' pour le scanner et le mise à jour. Par exemple :

* neuvector/manager:5.4.3
* neuvector/controller:5.4.3
* neuvector/enforcer:5.4.3
* neuvector/scanner:latest
* neuvector/updater:latest

Veuillez vous assurer de mettre à jour les références d'image dans les fichiers yaml appropriés.

Si vous déployez avec le chart Helm actuel {product-name} (v1.8.9+), les modifications suivantes doivent être apportées à values.yml :

* Mettre à jour le registre à docker.io
* Mettre à jour les noms/tags d'image à la version actuelle sur Docker Hub, comme indiqué ci-dessus
* Laissez les imagePullSecrets vides

== Déployer sur OpenShift

[,shell]
----
docker login docker.io
docker pull docker.io/neuvector/manager:<version>
docker pull docker.io/neuvector/controller:<version>
docker pull docker.io/neuvector/enforcer:<version>
docker pull docker.io/neuvector/scanner
docker pull docker.io/neuvector/updater
docker logout docker.io
----

Le fichier d'exemple ci-dessous déploiera un gestionnaire, 3 contrôleurs et 2 pods de scanner. Il déploiera un exécuteur sur chaque nœud en tant que daemonset, y compris sur le nœud maître (si planifiable). Voir la section inférieure pour spécifier des nœuds de gestionnaire ou de contrôleur dédiés en utilisant des étiquettes de nœud. Remarque : Il n'est pas recommandé de déployer (scaler) plus d'un gestionnaire derrière un équilibreur de charge en raison de problèmes potentiels d'état de session. Si vous prévoyez d'utiliser une réclamation PersistentVolume pour stocker la sauvegarde des fichiers de configuration {product-name}, veuillez consulter la section générale Sauvegarde/Données Persistantes dans l'aperçu de xref:production.adoc#_backups_and_persistent_data[Déploiement en Production].

Ensuite, définissez la route et autorisez les conteneurs {product-name} privilégiés en utilisant les instructions ci-dessous. Par défaut, OpenShift n'autorise pas les conteneurs privilégiés. De plus, par défaut, OpenShift ne planifie pas les pods sur le nœud maître. Voir les instructions à la fin pour activer/désactiver cela.

[NOTE]
====
Veuillez consulter la section Intégration d'Entreprise pour des détails sur l'intégration avec les Contrôles d'Accès Basés sur les Rôles (RBAC) d'OpenShift.
====

. Connectez-vous en tant qu'utilisateur normal
+
--
[,shell]
----
oc login -u <user_name>
----
--
. Créez un nouveau projet.
+
--
[NOTE]
====
Si l'argument --node-selector est utilisé lors de la création d'un projet, cela restreindra le placement des pods, comme pour l'exécuteur {product-name}, à des nœuds spécifiques.
====

[,shell]
----
oc new-project neuvector
----
--
. Poussez les images {product-name} vers le registre docker d'OpenShift.
+
--
[NOTE]
====
Pour OpenShift 4.6+, changez docker-registry.default.svc ci-dessous en image-registry.openshift-image-registry.svc dans les commandes ci-dessous
====

[,shell]
----
docker login -u <user_name> -p `+oc whoami -t+` docker-registry.default.svc:5000
docker tag docker.io/neuvector/enforcer:<version> docker-registry.default.svc:5000/neuvector/enforcer:<version>
docker tag docker.io/neuvector/controller:<version> docker-registry.default.svc:5000/neuvector/controller:<version>
docker tag docker.io/neuvector/manager:<version> docker-registry.default.svc:5000/neuvector/manager:<version>
docker tag docker.io/neuvector/scanner docker-registry.default.svc:5000/neuvector/scanner
docker tag docker.io/neuvector/updater docker-registry.default.svc:5000/neuvector/updater
docker push docker-registry.default.svc:5000/neuvector/enforcer:<version>
docker push docker-registry.default.svc:5000/neuvector/controller:<version>
docker push docker-registry.default.svc:5000/neuvector/manager:<version>
docker push docker-registry.default.svc:5000/neuvector/scanner
docker push docker-registry.default.svc:5000/neuvector/updater
docker logout docker-registry.default.svc:5000
----

[NOTE]
====
Veuillez consulter la section Mise à jour de la base de données CVE ci-dessous pour des recommandations sur la mise à jour de la dernière image de scanner dans votre registre.
====
--
. Connectez-vous en tant que compte system:admin
+
--
[,shell]
----
oc login -u system:admin
----
--
. Créez des comptes de service et accordez l'accès au SCC privilégié
+
--
[,shell]
----
oc create sa controller -n neuvector
oc create sa enforcer -n neuvector
oc create sa basic -n neuvector
oc create sa updater -n neuvector
oc create sa scanner -n neuvector
oc create sa registry-adapter -n neuvector
oc create sa cert-upgrader -n neuvector
oc -n neuvector adm policy add-scc-to-user privileged -z enforcer
----

Les informations suivantes seront ajoutées dans les utilisateurs SCC privilégiés :

[,yaml]
----
- system:serviceaccount:neuvector:enforcer
----

Ajoutez un nouveau neuvector-scc-controller scc pour le compte de service du contrôleur dans Openshift, en créant un fichier avec :

[,yaml]
----
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups: []
kind: SecurityContextConstraints
metadata:
  name: neuvector-scc-controller
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- ALL
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- azureFile
- projected
- secret
----

Puis appliquez

[,shell]
----
oc apply -f (filename)
----

Ensuite, exécutez la commande suivante pour lier le compte de service du contrôleur au scc neuvector-scc-controller

[,shell]
----
oc -n neuvector adm policy add-scc-to-user neuvector-scc-controller -z controller
----

Dans OpenShift 4.6+, utilisez ce qui suit pour vérifier :

[,shell]
----
oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide
----

[,shell]
----
NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS
system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/enforcer
----

Exécutez cette commande pour vérifier le service {product-name} pour le contrôleur :

[,shell]
----
oc get rolebinding system:openshift:scc:neuvector-scc-controller -n neuvector -o wide
----

La sortie ressemblera à

[,shell]
----
NAME                                            ROLE                                                        AGE     USERS   GROUPS   SERVICEACCOUNTS
System:openshift:scc:neuvector-scc-controller   ClusterRole/system:openshift:scc:neuvector-scc-controller   9m22s                    neuvector/controller
----
--
. Créez les ressources personnalisées (CRD) pour les règles de sécurité {product-name}. Pour OpenShift 4.6+ (Kubernetes 1.19+) :
+
--
[,shell]
----
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
----
--
. Ajoutez des autorisations de lecture pour accéder à l'API kubernetes et aux RBAC d'OpenShift. 
+
[IMPORTANT]
====
Le déploiement standard {product-name} 5.2+ utilise des comptes de service à privilèges minimaux au lieu de ceux par défaut. Voir ci-dessous si vous mettez à niveau vers 5.2+ à partir d'une version antérieure à 5.2.
====
+
--
[CAUTION]
========
Si vous mettez à niveau vers 5.3.0+, exécutez les commandes suivantes en fonction de votre version actuelle :

[tabs]
======
Version 5.2.0::
+
====
[,shell]
----
oc delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules
----
==== 

Versions antérieures à 5.2.0::
+
====
[,shell]
----
oc delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co oc delete rolebinding neuvector-admin -n neuvector
----
====
======
========

[,shell]
----
oc create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
oc create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,imagestreams.image.openshift.io
oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller
oc create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
oc create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
oc create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
oc create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view
oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller
oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller
oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-co --verb=get,list --resource=clusteroperators
oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller
oc create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
oc adm policy add-role-to-user neuvector-binding-secret system:serviceaccount:neuvector:controller system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:scanner system:serviceaccount:neuvector:registry-adapter -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
oc create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
oc create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
oc create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller
oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
oc create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
oc adm policy add-role-to-user neuvector-binding-cert-upgrader system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc adm policy add-role-to-user neuvector-binding-job-creation system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc adm policy add-role-to-user neuvector-binding-lease system:serviceaccount:neuvector:controller system:serviceaccount:neuvector:cert-upgrader -n neuvector --role-namespace neuvector
oc create clusterrole neuvector-binding-nvgroupdefinitions --verb=get,list,delete --resource=nvgroupdefinitions
oc create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
----
--
. Exécutez la commande suivante pour vérifier si les comptes de service neuvector/controller, neuvector/enforcer et neuvector/updater ont été ajoutés avec succès.
+
--
[,shell]
----
oc get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co -o wide
----

Sortie d'exemple :

[,shell]
----
NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               56d                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              34d                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         72d                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          72d                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   72d                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                72d                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                72d                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   72d                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                72d                    neuvector/controller
neuvector-binding-co                                ClusterRole/neuvector-binding-co                                72d                    neuvector/enforcer, neuvector/controller
----

Et cette commande :

[,shell]
----
oc get RoleBinding neuvector-binding-scanner neuvector-binding-cert-upgrader neuvector-binding-job-creation neuvector-binding-lease neuvector-binding-secret -n neuvector -o wide
----

Sortie d'exemple :

[,shell]
----
NAME                              ROLE                                   AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         56m                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   56m                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    56m                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           56m                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          56m                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter
----
--
. (*Optionnel*) Créez le Maître de la Fédération et/ou les Services de Gestion Multi-Cluster à Distance. Si vous prévoyez d'utiliser les fonctions de gestion multi-cluster dans {product-name}, un cluster doit avoir le service Maître de la Fédération déployé, et chaque cluster distant doit avoir le service Ouvrier de la Fédération. Pour plus de flexibilité, vous pouvez choisir de déployer à la fois les services Master et Worker sur chaque cluster afin que n'importe quel cluster puisse être un maître ou distant.
+
--
Services de gestion fédérés

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod
----

Ensuite, créez le(s) service(s) approprié(s) :

[,shell]
----
oc create -f nv_master_worker.yaml
----
--
. Créez les services et pods neuvector en fonction des exemples de yamls ci-dessous. 
+
[IMPORTANT] 
====
Remplacez les balises <version> pour les références d'image du gestionnaire, du contrôleur et de l'exécuteur dans le fichier yaml. Faites également toutes les autres modifications nécessaires pour votre environnement de déploiement.
====
+
--
[,shell]
----
oc create -f <compose file>
----
--

C'est tout ! Vous devriez pouvoir vous connecter à la console {product-name} et vous connecter avec admin:admin, par exemple `+https://<public-ip>:8443+`

Pour voir comment accéder à la console pour le service neuvector-webui :

[,shell]
----
oc get services -n neuvector
----

Si vous avez créé votre propre espace de noms au lieu d'utiliser &quot;`+neuvector+`&quot;, remplacez toutes les instances de &quot;`+namespace: neuvector+`&quot; et d'autres références d'espace de noms par votre espace de noms dans les fichiers yaml d'exemple ci-dessous.

*OpenShift 4.6+ avec CRI-O temps d'exécution*

Le nom de votre registre OpenShift par défaut pourrait avoir changé de docker-registry à openshift-image-registry. Vous devrez peut-être changer le registre d'images pour le gestionnaire, le contrôleur et l'exécuteur dans le yaml d'exemple.

[NOTE]
====
Le type NodePort est utilisé pour les services fed-master et fed-worker au lieu de LoadBalancer. Vous devrez peut-être ajuster pour votre déploiement.
====

Si vous utilisez le temps d'exécution CRI-O, consultez cet https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-crio-oc.yaml[exemple CRI-O].

*Noeud Maître Taints et Tolerations*

Toutes les informations de taint doivent correspondre pour planifier les exécuteurs sur les nœuds. Pour vérifier les informations de taint sur un nœud (par exemple, Maître) :

[,shell]
----
$ oc get node taintnodename -o yaml
----

Sortie d'exemple :

[,yaml]
----
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue
----

S'il y a des taints supplémentaires comme ci-dessus, ajoutez-les à la section des tolérances du yaml d'exemple :

[,yaml]
----
spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue
----

== Utilisation des étiquettes de nœud pour les nœuds Manager et Controller

Pour contrôler sur quels nœuds le Manager et le Controller sont déployés, étiquetez chaque nœud. Remplacez `+<nodename>+` par le nom de nœud approprié.

[,shell]
----
oc label nodes <nodename> nvcontroller=true
----

Ajoutez ensuite un nodeSelector au fichier yaml pour les sections de déploiement du Manager et du Controller. Par exemple :

[,yaml]
----
          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always
----

Pour empêcher l'enforcer d'être déployé sur un nœud controller, s'il s'agit d'un nœud de gestion dédié (sans conteneurs d'application à surveiller), ajoutez une nodeAffinity à la section yaml de l'Enforcer. Par exemple :

[,yaml]
----
app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:
----

== Mise à jour de la base de données CVE sur les déploiements OpenShift

L'image du scanner la plus récente contient toujours la mise à jour de la base de données CVE la plus récente de {product-name}. Pour cette raison, un tag de version n'est pas recommandé lors du tirage de l'image. Cependant, la mise à jour de la base de données CVE nécessite un tirage régulier de la dernière image du scanner afin que le cron job de mise à jour puisse redéployer le(s) scanner(s).  Les exemples ci-dessus supposent que les images {product-name} sont tirées, taguées et poussées vers un registre OpenShift local. Le déploiement se fait alors à partir de ce registre au lieu de directement depuis neuvector (ou le registre {product-name} hérité sur docker hub).

Pour mettre à jour régulièrement la base de données CVE, nous recommandons de créer un script/cron job pour tirer la dernière image du scanner {product-name} et effectuer les étapes de tagging et de pushing vers le registre local. Cela garantira que la base de données CVE est mise à jour régulièrement et que les images et conteneurs sont scannés pour de nouvelles vulnérabilités.

== Mises à jour progressives

Les outils d'orchestration tels que Kubernetes, RedHat OpenShift et Rancher prennent en charge les mises à jour progressives avec des politiques configurables. Vous pouvez utiliser cette fonctionnalité pour mettre à jour les conteneurs {product-name}. Le plus important sera de s'assurer qu'il y a au moins un Allinone/Controller en cours d'exécution afin que les politiques, les journaux et les données de connexion ne soient pas perdus. Assurez-vous qu'il y a un minimum de 30 secondes entre les mises à jour des conteneurs afin qu'un nouveau leader puisse être élu et que les données soient synchronisées entre les contrôleurs.

Avant de commencer les mises à jour progressives, veuillez tirer et taguer les conteneurs {product-name} de la même manière qu'au début de cette page. Vous pouvez tirer la dernière version sans numéro de version, mais pour déclencher la mise à jour progressive, vous devrez taguer l'image avec une version.

Par exemple, pour le contrôleur (dernier) :

[,shell]
----
docker pull neuvector/controller
----

Ensuite, pour taguer/pousser, si la dernière version est 2.0.1, comme à l'étape 3 en haut de cette page :

[,shell]
----
docker login -u <user_name> -p `+oc whoami -t+` docker-registry.default.svc:5000
docker tag neuvector/controller docker-registry.default.svc:5000/neuvector/controller:2.0.1
docker push docker-registry.default.svc:5000/neuvector/controller:2.0.1
----

Vous pouvez maintenant mettre à jour votre fichier yaml avec ces nouvelles versions et &apos;`+apply+`&apos;, ou utiliser la commande &apos;`+oc set image ...+`&apos; pour déclencher la mise à jour progressive. Veuillez consulter les exemples de mise à jour progressive de Kubernetes dans cette section Production pour savoir comment lancer et surveiller les mises à jour progressives des conteneurs {product-name}.

Les fichiers yaml de déploiement fournis configurent déjà la politique de mise à jour progressive. Si vous mettez à jour via le chart Helm {product-name}, veuillez tirer le dernier chart pour configurer correctement de nouvelles fonctionnalités telles que le contrôle d'admission, et supprimer l'ancien rôle de cluster et le lien de rôle de cluster pour {product-name}.

== Activation de l'API REST

Pour activer l'API REST, le port 10443 doit être configuré comme suit :

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod
----

== Activer/Désactiver la planification sur le nœud maître

Les commandes suivantes peuvent être utilisées pour activer/désactiver la planification sur le nœud maître.

[,shell]
----
oc adm manage-node nodename --schedulable
----

[,shell]
----
oc adm manage-node nodename --schedulable=false
----

== Déploiement OpenShift en mode non privilégié

Les instructions suivantes peuvent être utilisées pour déployer {product-name} sans utiliser de conteneurs en mode privilégié. Le contrôleur est déjà en mode non privilégié et le déploiement de l'enforcer doit être modifié, comme le montre les extraits ci-dessous.

Enforcer :

[,yaml]
----
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # this line below is required to be added if k8s version is pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # openshift
            seLinuxOptions:
              type: unconfined_t
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
----

L'exemple suivant est une référence de déploiement complète utilisant le runtime cri-o. Pour d'autres runtimes, veuillez apporter les modifications appropriées aux volumes/montages de volume pour le crio.sock.

.Cliquez ici pour plus de détails
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: ClusterIP
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: neuvector-route-webui
  namespace: neuvector
spec:
  to:
    kind: Service
    name: neuvector-service-webui
  port:
    targetPort: manager
  tls:
    termination: passthrough

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/manager:<version>
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/controller:<version>
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # - name: CTRL_PERSIST_CONFIG
            #   value: "1"
          volumeMounts:
            # - mountPath: /var/neuvector
            #   name: nv-share
            #   readOnly: false
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        # - name: nv-share
        #   persistentVolumeClaim:
        #     claimName: neuvector-data
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/enforcer:<version>
          securityContext:
            # openshift
            seLinuxOptions:
              type: unconfined_t
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            # - mountPath: /run/runtime.sock
            #   name: runtime-sock
            #   readOnly: true
            # - mountPath: /host/proc
            #   name: proc-vol
            #   readOnly: true
            # - mountPath: /host/cgroup
            #   name: cgroup-vol
            #   readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        # - name: runtime-sock
        #   hostPath:
        #     path: /var/run/crio/crio.sock
        # - name: proc-vol
        #   hostPath:
        #     path: /proc
        # - name: cgroup-vol
        #   hostPath:
        #     path: /sys/fs/cgroup
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: image-registry.openshift-image-registry.svc:5000/neuvector/scanner:<version>
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: image-registry.openshift-image-registry.svc:5000/neuvector/updater:<version>
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`+cat /var/run/secrets/kubernetes.io/serviceaccount/token+`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"&apos;`+date +%Y-%m-%dT%H:%M:%S%z+`&apos;"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====
